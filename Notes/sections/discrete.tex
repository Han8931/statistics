\chapter{Discrete Random Variables}
\label{ch:discrete}

\section{Random Variables}
A random variable $X$ is a function from the sample space to the real numbers.
$$X:S\to \mathbb{R}$$

\section{Probability Mass Function (PMF)}
If $X$ is a discrete random variable then its range $R_X$ is a countable set, so, we can list the elements in $R_X$. In other words, we can write 
$$R_X=\{x_1,x_2,\dots\}$$
Note that here $x_1,x_2,\dots$ are possible values of the random variable $X$. While random variables are usually denoted by capital letters, to represent the numbers in the range we usually use lowercase letters. For a discrete random variable $X$, we are interested in knowing the probabilities of $X=x_k$. 

Let $X$ be a discrete random variable with range $R_X=\{x_1,x_2,\dots\}$ (finite or countably infinite). The function
$$P_X(x_k)=P(X=x_k), \,\text{for }k=1,2,3\dots $$
is called the probability mass function (PMF) of $X$. 

\section{Special Distributions}

\subsection{Bernoulli Distribution}
\begin{align*}
	P_X(x) = \begin{cases}
	p & \text{for } x=1\\
	1-p & \text{for } x=0\\
	0 & \text{Otherwise}
\end{cases}
\end{align*}
A Bernoulli random variable is associated with a certain event $A$. If event $A$ occurs (for example, if you pass the test), then $X=1$; otherwise $X=0$. For this reason the Bernoulli random variable, is also called the \textit{indicator} random variable.


\subsection{Geometric Distribution}
Suppose that I have a coin with $P(H)=p$. I toss the coin until I observe the first heads. We define $X$ as the total number of coin tosses in this experiment. Then $X$ is said to have geometric distribution with parameter $p$. In other words, you can think of this experiment as \textbf{repeating independent Bernoulli trials until observing the first success}. The range of $X$ here is $RX=\{1,2,3,\dots\}$. 
$$P_X(k) =P(X=k)=(1-p)^{k-1} p, \textrm{ for } k=1,2,3,\dots$$

\subsection{Binomial Distribution}
Suppose that I have a coin with $P(H)=p$. I toss the coin $n$ times and define $X$ to be the total number of heads that I observe. Then $X$ is binomial with parameter $n$ and $p$. The range of $X$ in this case is $R_X={0,1,2,\dots,n}$. 
$$P_X(k) = P(X=k) =\binom{n}{k}p^k(1-p)^{n-k}.$$

Here is a useful way of thinking about a binomial random variable. It can be obtained by $n$ independent coin tosses. If we think of each coin toss as a Bernoulli random variable, the $Binomial(n,p)$ random variable is a sum of $n$ independent $Bernoulli(p)$ random variables. This is stated more precisely in the following lemma.

If $X_1,X_2,\dots,X_n$ are independent $Bernoulli(p)$ random variables, then the random variable $X$ defined by $X=X_1+X_2+\dots+X_n$ has a $Binomial(n,p)$ distribution. 


Example: 
\begin{itemize}
	\item Let $X\sim Binomial(n,p)$ and $Y\sim Binomial(m,p)$ be two independent random variables. Define a new random variable as $Z=X+Y$. Find the PMF of $Z$.
	\item Solution 1: Since $X\sim Binomial(n,p)$, we can think of $X$ as the number of heads in $n$ independent coin tosses: 
	$$X = X_1+\dots+X_n,$$
	where the $X_i$'s are independent Bernoulli RVs. Similarly, $Y\sim Binomial(m,p)$.
	Thus, the RV $Z=X+Y$ will be the total number of heads in $n+m$ coin tosses:
	$$Z = X+Y=X_1+\dots+X_n+Y_1+\dots+Y_m.$$
	Therefore, $Z$ is a binomial RV with parameters $m+n$ and $p$, \ie $Binomial(m+n, p)$.
	\item Solution 2: First, we note that $R_z = \{0,1,\dots,m+n\}$. For $k\in R_z$, we get
		$$P_Z(k) = P(Z=k) = P(X+Y=k).$$
		We will find $P(X+Y=k)$ by using conditioning and the law of total probability. 
		\begin{align*}
			P(Z=k) &= P(X+Y=k)\\
				   &= \sum_{i=0}^{n}P(X+Y=k|X=i)P(X=i)\\
				   &= \sum_{i=0}^{n}P(Y=k-i|X=i)P(X=i)\\
				   &= \sum_{i=0}^{n}P(Y=k-i)P(X=i)\quad \text{Since $X$ and $Y$ are independent}\\
				   &= \sum_{i=0}^{n}\binom{m}{k-i}p^{k-i}(1-p)^{m-k+i}\binom{n}{i}p^i(1-p)^{n-i}\\
				   &= \sum_{i=0}^{n}\binom{m}{k-i}\binom{n}{i}p^k(1-p)^{m+n-k}\\
				   &= p^k(1-p)^{m+n-k}\sum_{i=0}^{n}\binom{m}{k-i}\binom{n}{i}\\
				   &= \binom{m+n}{k}p^k(1-p)^{m+n-k}\quad \text{by Vandermonde's identity}
		\end{align*}
\end{itemize}

\paragraph{Negative Binomial (Pascal) Distribution}
The \textit{negative binomial} or \textit{Pascal distribution} is a \textbf{generalization of the geometric distribution}. It relates to the random experiment of \textbf{repeated independent trials until observing $m$ successes}. Suppose that I have a coin with $P(H)=p$. I toss the coin until I observe $m$ heads, where $m\in \mathbb{N}$. We define $X$ as the total number of coin tosses in this experiment. Then $X$ is said to have Pascal distribution with parameter $m$ and $p$. We write $X\sim Pascal(m,p)$. Note that $Pascal(1,p=Geometric(p))$, since the geometric distribution repeats trials until observing the first success. Note that by our definition the range of $X$ is given by $R_X=\{m, m+1, m+2, m+3, \dots\}$, since $X$ is the number of coin tosses to observe $m$ target events.

Let's derive the PMF of a $Pascal(m,p)$ RV $X$. To find the probability of the event $A = \{X=k\}$, we argue as follows. By definition, event $A$ can be written as $A=B\cap C,$ where
\begin{itemize}
	\item $B$ is the event that we observe $m-1$ heads (\ie successes) in the first $k-1$ trials
	\item $C$ is the event that we observe a head in the $k$-th trial.
\end{itemize}

Note that $B$ and $C$ are independent events because they are related to different independent trials (coin tosses). Thus,
$$P(A)=P(B\cap C) = P(B)P(C).$$
We get $P(C) = p$, so 
\begin{align*}
	P(B) = \binom{k-1}{m-1}p^{m-1}(1-p)^{(k-1)-(m-1)} = \binom{k-1}{m-1}p^{m-1}(1-p)^{k-m}.
\end{align*}
Finally, we obtain
\begin{align*}
	P(B) = \binom{k-1}{m-1}p^{m}(1-p)^{k-m}.
\end{align*}

\subsection{Hyper-Geometric Distribution}
You have a bag that contains $b$ blue marbles and $r$ red marbles. You choose $k\leq b+r$ marbles at random (without replacement). Let $X$ be the number of blue marbles in your sample. By this definition, we have $X\leq \min(k,b)$. Also, the number of red marbles in your sample must be less than or equal to $r$, so we conclude $X\geq \max(0,k-r)$. Therefore, the range of $X$ is given by $R_X=\{\max(0,k-r),\max(0,k-r)+1,\max(0,k-r)+2,\dots,\min(k,b)\}$.

To find $P_X(x)$, note that total number of ways to choose $k$ marbles from $b+r$ marbles is $\binom{b+r}{k}$. The total number of ways to choose $x$ blue marbles and $k-x$ red marbles is $\binom{b}{x}\binom{r}{k-x}$. Thus, we get
\begin{align*}
	P_X(x)= \frac{{b \choose x} {r \choose k-x}}{{b+r \choose k}}, \quad \text{ for } x \in R_X.
\end{align*}
\subsection{Poisson Distribution}

The Poisson distribution is one of the most widely used probability distributions. It is usually used in scenarios where we are \textbf{counting the occurrences of certain events in an interval of time or space}. In practice, it is often an approximation of a real-life random variable. Here is an example of a scenario where a Poisson random variable might be used. Suppose that we are counting the number of customers who visit a certain store from 1pm to 2pm. Based on data from previous days, we know that on average $\lambda=15$ customers visit the store. Of course, there will be more customers some days and fewer on others. Here, we may model the random variable $X$ showing the number customers as a Poisson random variable with parameter $\lambda=15$. Let us introduce the Poisson PMF first, and then we will talk about more examples and interpretations of this distribution.
\begin{align*}
	P_X(k) = e^{-\lambda}\frac{\lambda^{k}}{k!}.
\end{align*}
Note that $\lambda$ is the mean number of events within a given interval of time or space. 

Example: The number of emails that I get in a weekday can be modeled by a Poisson distribution with an average of 0.2 emails per minute. 
\begin{itemize}
	\item What is the probability that I get no emails in an interval of length 5 minutes?
		\begin{itemize}
			\item For 5 minutes, there would be 1 email on average. Thus, $\lambda=1$, 
				$$P(X=0) = P_X(0) = e^{-\lambda}\frac{\lambda^{0}}{0!} = \frac{1}{e}\approx 0.37$$
		\end{itemize}
	\item What is the probability that I get more than 3 emails in an interval of length 10 minutes?
		\begin{itemize}
			\item Let $Y$ be the number of emails that I get in the 10-minute interval. Then by the assumption $Y$ is a Poisson RV with parameter $\lambda = 10\times 0.2 = 2$. Thus,
				\begin{align*}
					P(Y>3) &= 1-P(Y\leq 3)\\
						   &= 1-(P_Y(0)+P_Y(1)+P_Y(2)+P_Y(3))\\
						   &= 1-e^{-\lambda}-\frac{e^{-\lambda} \lambda}{1!}-\frac{e^{-\lambda} \lambda^2}{2!}-\frac{e^{-\lambda} \lambda^3}{3!}\\
						   &\approx 0.1429
				\end{align*}
		\end{itemize}
\end{itemize}

Imagine you have a busy customer service center that receives phone calls. You want to know how many calls to expect in an hour, but calls can come at any moment and don't follow a strict schedule. 
\begin{itemize}
	\item Average Rate ($\lambda$): First, you determine the average number of calls you receive per hour. Let's say itâ€™s 10 calls per hour. This average rate is denoted by the symbol $\lambda$ 
	\item Probability Calculation: Using the Poisson formula, you can calculate the probability of receiving a certain number of calls in any given hour.
\end{itemize}
\[
P(X = k) = \frac{e^{-\lambda} \lambda^k}{k!}
\]
\begin{itemize}
	\item \( P(X = k) \) is the probability of getting \( k \) calls in an hour.
	\item \( e \) is the base of the natural logarithm (approximately equal to 2.71828).
	\item \( \lambda \) is the average rate (10 calls per hour).
	\item \( k \) is the number of calls you want to find the probability for.
	\item \( k! \) (k factorial) is the product of all positive integers up to \( k \).
\end{itemize}


\subsection{Poisson as an Approximation for Binomial}
 
The Poisson distribution can be viewed as the limit of binomial distribution. Suppose $X\sim Binomial(n,p)$ where the number of trials $n$ is very large and the probability of success $p$ is very small. In particular, assume that $\lambda=np$ is a positive constant. We show that the PMF of $X$ can be approximated by the PMF of a $Poisson(\lambda)$ random variable. The importance of this is that Poisson PMF is much easier to compute than the binomial. Let us state this as a theorem.

Let $X\sim Binomial(n,p=\frac{\lambda}{n})$, where $\lambda>0$ is fixed. Then for any $k\in\{0,1,2,\dots\}$ we have 
$$\lim_{n\to \infty}P_X(k) = \frac{e^{-\lambda} \lambda^k}{k!}.$$

\href{https://www.probabilitycourse.com/chapter3/3_1_5_special_discrete_distr.php}{References Poisson}

\section{Cumulative Distribution Function}
\label{sec:cdf}
The PMF is one way to describe the distribution of a discrete RV. As we will see later on, PMF cannot be defined for continuous random variables. The cumulative distribution function (CDF) of a random variable is another method to describe the distribution of random variables. The advantage of the CDF is that it can be defined for any kind of RV (discrete, continuous, and mixed).

\begin{definition}{Cumulative Distribution Function}
	The cumulative distribution function (CDF) of random variable $X$ is defined as 
	$$F_X(x) = P(X\leq x), \forall x\in\mathbb{R}.$$
\end{definition}
Note that the subscript $X$ indicates that this is the CDF of the random variable $X$. Also, note that the CDF is defined for all $x\in \mathbb{R}$. 

\section{Expectation}
If you have a collection of numbers $a_1,a_2,\dots,a_N$, their average is a single number that describes the whole collection. Now, consider a random variable $X$. We would like to define its average, or as it is called in probability, its expected value or mean. The expected value is defined as the weighted average of the values in the range.

\begin{definition}{Expected Value}
	Let $X$ be a discrete RV with range $R_X = \{x_1,x_2,\dots,\}$. The expected value of $X$, denoted by $EX$ is defined as
	$$EX = \sum_{x_k\in R_X}x_kP(X=x_k) = \sum_{x\in R_X}x_kP_X(x_k)$$
\end{definition}

\section{Functions of Random Variables}
If $X$ is a RV and $Y=g(X)$, then $Y$ itself is a random variable. Thus, we can talk about its PMF, CDF, and expected value. First note that the range of $Y$ can be written as 
$$R_Y = \{g(x)|x\in R_X\}.$$
If we already know the PMF of $X$, to find the PMF of $Y=g(X)$, we can write
\begin{align*}
	P_Y(y) &= P(Y=y)\\
		   &= P(g(X)=y)\\
		   &= \sum_{x:g(x)=y}P_X(x)
\end{align*}

\paragraph{Example:} Let $X$ be a discrete RV with $P_X(k) = \frac{1}{5}$ for $k=-1,0,1,2,3$. Let $Y=2|X|$. Find the range and PMF of $Y$.
The range of $Y$ is 
\begin{align*}
	R_Y &= \{2|x|\}\\
		&= \{0,2,4,6\}
\end{align*}
To find $P_Y(y)$, we need to find $P(Y=y)$ for $y=0,2,4,6$:
\begin{align*}
	P_Y(0) &= P(Y=0)  = P(2|x|=0)\\
		   &= P(X=0) = \frac{1}{5}\\
	P_Y(2) &= P(Y=2)  = P(2|x|=2)\\
		   &= P(X=-1 \text{ or } X=1)\\
		   &= P_X(-1) + P_X(1) = \frac{2}{5}\\
		   \vdots
\end{align*}

\subsection{Expected Value of a Function of a Random Variable (LOTUS)}

Let $X$ be a discrete random variable with PMF $P_X(x)$, and let $Y=g(X)$. Suppose that we are interested in finding $EY$. One way to find $EY$ is to first find the PMF of $Y$ and then use the expectation formula $EY=E[g(X)]=\sum_{y\in R_y}yP_Y(y)$. But there is another way which is usually easier. It is called the \textit{law of the unconscious statistician} (LOTUS).
$$\mathbb{E}[g(X)]=\sum_{x_k\in R_X}g(x_k)P_{X}(x_k)$$
One of the main points of the theorem is that you can compute $\mathbb{E}[g(X)]$ without computing $P_Y(y)$. In practice it is usually easier to use LOTUS than direct definition when we need $\mathbb{E}[g(X)]$.

\section{Variance}
The variance of a random variable $X$, with mean $EX=\mu_X$, is defined as
$$\text{Var}(X) = \mathbb{E}[(X-\mu_X)^2].$$
To compute $\text{Var}(X) = \mathbb{E}[(X-\mu_X)^2]$, note that we need to find the expected value of $g(X)=(X-\mu_X)^2$, so we can use \textbf{LOTUS}. In particular, we can write 
$$\text{Var}(X) = \mathbb{E}[(X-\mu_X)^2]= \sum_{x_k\in R_X}(x_k-\mu_X)^2P_X(x_k).$$

\section{Standard Deviation}
$$\textrm{SD}(X)= \sigma_X= \sqrt {\textrm{Var}(X)}.$$

A useful formula for computing the variance is 
$$\text{Var}(X) = \mathbb{E}[X^2]-\mathbb{E}[X]^2.$$
We can find $\mathbb{E}[X^2]$ using LOTUS: 
$$\mathbb{E}[X^2] = \sum_{x_k\in R_X}x_k^2P_X(x_k).$$




% \begin{itemize}
% 	\item Bernoulli:
% 		$$EX = 0\cdot P_X(0)+1\cdot P_X(1)=p$$
% \end{itemize}
