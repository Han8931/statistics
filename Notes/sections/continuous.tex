\chapter{Continuous and Mixed Random Variables}

\section{Introduction}
Remember that discrete random variables can take only a countable number of possible values. On the other hand, a continuous random variable $X$ has a range in the form of an interval or a union of non-overlapping intervals on the real line (possibly the whole real line). Also, for any $x\in \mathbb{R}$, $P(X=x)=0$. Thus, we need to develop new tools to deal with continuous random variables. The good news is that the theory of continuous random variables is completely analogous to the theory of discrete random variables. Indeed, if we want to oversimplify things, we might say the following: take any formula about discrete random variables, and then replace sums with integrals, and replace PMFs with probability density functions (PDFs), and you will get the corresponding formula for continuous random variables. 

\section{Probability Density Function (PDF)}

To determine the distribution of a discrete random variable we can either provide its PMF or CDF. For continuous random variables, the CDF is well-defined so we can provide the CDF. However, \textbf{the PMF does not work for continuous random variables, because for a continuous random variable} $P(X=x)=0,\ \forall x\in \mathbb{R}$. Instead, we can usually define the probability density function (PDF). The PDF is the $density$ of probability rather than the probability mass. The concept is very similar to mass density in physics: its unit is probability per unit length. To get a feeling for PDF, consider a continuous random variable $X$ and define the function $f_X(x)$ as follows (wherever the limit exists):
$$f_X(x)=\lim_{\Delta \rightarrow 0^+} \frac{P(x < X \leq x+\Delta)}{\Delta}.$$
The function $f_X(x)$ gives us the probability density at point $x$. It is the limit of the probability of the interval $(x,x+\Delta]$ divided by the length of the interval as the length of the interval goes to $0$. Remember that 
$$P(x<X\leq x+\Delta) = F_X(x+\Delta)-F_X(x).$$
Thus, we get
\begin{align*}
	f_X(x)&=\lim_{\Delta \rightarrow 0} \frac{F_X(x+\Delta)-F_X(x)}{\Delta}\\
		  &= \frac{dF_X(x)}{dx}\\
		  &= F'_X(x), \quad \text{if }F_X(x)\text{ is differentiable at }x.
\end{align*}

Let's find the PDF of the uniform random variable $X\sim Uniform(a,b)$, which can be expressed as follows: 
\begin{equation*}
	f_X(x) = \left\{
	\begin{array}{l l}
	\frac{1}{b-a} & \quad a < x < b\\
	0 & \quad x < a \textrm{ or } x > b
	\end{array} \right.
\end{equation*}
Note that the CDF is not differentiable at points $a$ and $b$. Nevertheless, this is not important at this moment. 

The uniform distribution is the simplest continuous random variable you can imagine. For other types of continuous random variables the PDF is non-uniform. Note that for small values of $\delta$ we can write 
$$P(x < X \leq x+\delta) \approx f_X(x) \delta.$$
Thus, if $f_X(x_1)<f_X(x_2)$, we can say $P(x_1 < X \leq x_1+\delta)<P(x_2 < X \leq x_2+\delta)$ , \ie the value of $X$ is more likely to be around $x_2$ than $x_1$. 

Since the PDF is the derivative of the CDF, the CDF can be obtained from PDF by integrations (by assuming absolute continuity):
$$F_X(x) = \int_{-\infty}^xf_X(u)du.$$
Also, we have
$$P(a < X \leq b) = F_X(b)-F_X(a) = \int_{a}^bf_X(u)du.$$
More generally, for a set $A$, $P(X\in A) = \int_a^b f_X(u)du.$ Note that if we integrate over the entire real line, we must get 1, \ie
$$\int_{-\infty}^{\infty}f_X(u)du = 1.$$

\section{Expected Value and Variance}
As we mentioned earlier, the theory of continuous random variables is very similar to the theory of discrete random variables. In particular, usually summations are replaced by integrals and PMFs are replaced by PDFs. The proofs and ideas are very analogous to the discrete case, so sometimes we state the results without mathematical derivations for the purpose of brevity.

Recall that the expected value of a discrete random variable can be obtained as
$$EX = \sum_{x_k\in R_X}x_k P_X(x_k).$$

The expected value of a continuous RV as
$$EX = \int_{-\infty}^{\infty}x f_X(x)dx.$$

\subsection{Expected Value of a Function of a Continuous Random Variable}
Law of the unconscious statistician (LOTUS) for continuous random variables:
$$\mathbb{E}[g(X)] = \int_{-\infty}^{\infty}g(x) f_X(x)dx$$

\subsection{Variance}
\begin{align*}
	\text{Var}(X) &= \mathbb{E}[(X-\mu_X)^2] \\
				  &= \int_{-\infty}^{\infty}(x-\mu_X)^2 f_X(x)dx\\
				  &= EX^2-(EX)^2\\
				  &= \int_{-\infty}^{\infty}x^2 f_X(x)dx-\mu_X^2
\end{align*}
Note that for $a,b\in \mathbb{R}$, we always have
$$\text{Var}(aX+b) = a^2\text{Var}(X)$$


\section{Functions of Continuous Random Variables}

If $X$ is a continuous random variable and $Y=g(X)$ is a function of $X$, then $Y$ itself is a random variable. Thus, we should be able to find the CDF and PDF of $Y$. It is usually more straightforward to start from the CDF and then to find the PDF by taking the derivative of the CDF. Note that before differentiating the CDF, we should check that the CDF is continuous. As we will see later, the function of a continuous random variable might be a non-continuous random variable. Let's look at an example.

\paragraph{Example:} Let $X$ be a $Uniform(0,1)$ random variable, and let $Y=e^X$.
\begin{itemize}
	\item CDF of $Y$
	\item PDF of $Y$
	\item $EY$
\end{itemize}

The PDF of $X$ is given by

\begin{align*}
	F_X(x) =
	\begin{cases}
		0&\text{for}\ x<0\\
		x&\text{for}\ 0\leq x\leq 1\\
		1&\text{for}\ x>1
	\end{cases}
\end{align*}

The range of $x$, $R_X=[0,1]$, so the range of $Y$, $R_Y=[1,e]$. We can find the CDF of $Y$ as follows:
\begin{align*}
	F_Y(y) &= P(Y\leq y)\\ 
		   &= P(e^X\leq y)\\ 
		   &= P(X\leq \ln y) \quad \text{, since }e^x \text{ is an increasing function.}\\ 
		   &= F_X(\ln y) \quad \text{by definition.}\\
		   &= \ln y \quad \text{, since } F_X(x)=x \text{ for } 0\leq x \leq 1 \text{ and } 0\leq \ln y \leq 1. 
\end{align*}

\section{The Method of Transformations}
So far, we have discussed how we can find the distribution of a function of a continuous random variable starting from finding the CDF. If we are interested in finding the PDF of $Y=g(X)$, and the function $g$ satisfies following properties, it might be easier to use a method called the method of transformations.
\begin{itemize}
	\item $g(x)$ is differentiable;
	\item $g(x)$ is a strictly increasing function, that is, if $x_1<x_2$, then $g(x_1)<g(x_2)$.
\end{itemize}

Now, let $X$ be a continuous random variable and $Y=g(X)$. We will show that you can directly find the PDF of $Y$ using the following formula.
\begin{equation*}
	f_Y(y) = 
	\begin{cases}
	\frac{f_X(x_1)}{g'(x_1)}=f_X(x_1). \frac{dx_1}{dy} & \quad \textrm{where } g(x_1)=y\\
	0 & \quad \textrm{if }g(x)=y \textrm{ does not have a solution}
	\end{cases} 
\end{equation*}
Note that since $g$ is strictly increasing, its inverse function $g^{-1}$ is well defined. That is, for each $y\in R_Y$, there exists a \textbf{unique} $x_1$ such that $g(x_1)=y$. We can write $x_1=g^{-1}(y)$.
\begin{align*}
	F_Y(y) &= P(Y\leq y)\\
		   &= P(g(X)\leq y)\\
		   &= P(X< g^{-1}(y))\quad \text{since }g \text{ is strictly increasing.}\\
		   &= F_X(g^{-1}(y)).
\end{align*}
To find the PDF of $Y$, we differente,
\begin{align*}
	f_Y(y) &= \frac{d}{dy}F_X(x_1)\quad \text{by } g(x_1)=y\\
		   &=\frac{dx_1}{dy}\cdot \underbrace{\frac{d}{dx_1}F_X(x_1)}_{=F'_X(x_1)}\\
		   &=\frac{dx_1}{dy}f_X(x_1)\\
		   &= \frac{f_X(x_1)}{g'(x_1)} \quad \text{,since } \frac{dx}{dy}=\frac{1}{\frac{dy}{dx}}.
\end{align*}
We can repeat the same argument for the case where $g$ is \textbf{strictly decreasing}. In that case, $g'(x_1)$ will be \textbf{negative}, so we need to use $|gâ€²(x_1)|$ . Thus, we can state the following theorem for a \textit{strictly monotonic function}. (A function $g:R\to R$ is called strictly monotonic if it is strictly increasing or strictly decreasing.)



















% \subsection{Intuitive Explanation}
% ``How to derive the PDF of the random variable $Y=g(X)$ when one knows the pdf of the random variable $X$?''. For a general function $g$, there is no direct formula to get the pdf of the random variable $Y=g(X)$ knowing the PDF of $X$. There is a formula in case when $h$ is a differentiable one-to-one mapping from the range (the support, I should say) of $X$ to the range of $Y$.

% Take for example a random variable $X\sim \mathcal{N}(\mu, \sigma)$ and set $Y=\exp(X)$. The figure below shows some simulations of $X$ and the corresponding values of $Y$. The density of $X$ is shown in blue and the one of $Y$ is shown in orange in the vertical direction.
% \begin{figure}[ht]
%     \centering
%     \begin{minipage}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./images/sample.png}
%         % \caption{Caption for the first figure}
%         % \label{fig:figure1}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./images/sample2.png}
%         % \caption{Caption for the second figure}
%         % \label{fig:figure2}
%     \end{minipage}
%     % \caption{Overall caption for the two figures}
%     % \label{fig:figures}
% \end{figure}
% Now the question is: knowing the density of $X$, what is the density of $Y$?
% Taking a point $y$ in the range of $Y$, the PDF $f_Y$ provides the probability of $Y$, belong to a small area $dy$ around $y$ by the formula below
% $$P(Y\in dy)\approx f_Y(y)|dy|,$$
% where $P(Y\in dy)$ is the area below the curve. Similarly, we can define
% $$P(X\in dx)\approx f_X(x)|dx|$$
% The above two areas are approximately the same in case of very small region. Note that if $dy$ and $dx$ are very small, we can approximate the derivative of $g'(x)=\frac{|dy|}{|dx|}$. Compactly, this can be expressed as follows:
% $$P(Y\in dy) = P(X\in dx) = f_X(x)\frac{|dy|}{g'(x)}$$
% With $y=g(x)$ we can get 
% \begin{align*}
% 	P(Y\in dy) &= f_X(x)\frac{|dy|}{g'(x)}\\
% 	& = f_X(g^{-1}(y))\frac{|dy|}{g'(g^{-1}(y))}\\
% 	& = f_X(g^{-1}(y))|dy|(g^{-1})'(y)
% \end{align*}
% The last line is by the derivative of inverse function which is 
% $$\frac{d}{dx}f^{-1}(x) = \frac{1}{f'(f^{-1}(x))}$$
% Finally, we can get 
% $$f_Y(y) = f_X(g^{-1}(y))|(g^{-1})'(y)|$$
% Note that the absolute is determined by the function $h$. This is the so-called change of variables formula.


