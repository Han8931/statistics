\chapter{Multiple Random Variables}

\section{Joint Distributions and Independence}

For three or more random variables, the joint PDF, joint PMF, and joint CDF are defined in a similar way to wat we have already seen for the case of two random variables. Let $X_1,\dots,Xn$ be $n$ discrete random variables. The joint PMF of $X_1,\dots,X_n$ is defined as
$$P_{X_1,\dots X_n}(x_1,\dots, x_n) = P(X_1=x_1,\dots,X_n=x_n).$$

For $n$ jointly continuous random variables $X_1,\dots,Xn$ the joint PDF is defined to be the function $f_{X_1,\dots,Xn}(x_1,\dots,x_n)$ such that the probability of any set $A\subset \mathbb{R}^n $ is given by the integral of the PDF over the set $A$. In particular, for a set $A\in \mathbb{R}^n$, we can write 
$$P\left( (X_1,\dots X_n)\in A \right)\int \cdots \int_A \cdots \int f_{X_1,\dots,X_n}(x_1,\dots, x_n) dx_1,\dots,dx_n.$$

The marginal PDF of $X_i$ can be obtained by integrating all other $X_j$'s. For example,
$$\int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} f_{X_1,\dots,X_n}(x_1,\dots, x_n) dx_1,\dots,dx_n.$$

The joint CDF of $n$ random variables $X_1,\dots, X_n$ is defined as
$$F_{X_1,\dots,X_n}(x_1,\dots, x_n) = P(X_1\leq x_1, \dots, X_n\leq x_n).$$

\paragraph{Independence} The idea of Independence is exactly the same as what we have seen before:
\begin{itemize}
	\item $F_{X_1,\dots,X_n}(x_1,\dots, x_n) = F_{X_1}(x_1)F_{X_2}(x_2)\cdots F_{X_n}(x_n).$
	\item Equivalently, if $X_1,\dots, X_n$ are discrete, then they are independent if for all 
		$$P_{X_1,\dots,X_n}(x_1,\dots, x_n) = P_{X_1}(x_1)P_{X_2}(x_2)\cdots P_{X_n}(x_n).$$
	\item If $X_1,\dots, X_n$ are continuous, then they are independent if for all 
		$$f_{X_1,\dots,X_n}(x_1,\dots, x_n) = f_{X_1}(x_1)f_{X_2}(x_2)\cdots f_{X_n}(x_n).$$
	\item If random variables are independent, 
		$$E[X_1,\dots,X_n] = E[X_1]\dots E[X_n]. $$
\end{itemize}

If random variables $X_1,\dots,X_n$ are independent and identically distributed (i.i.d.) then they will have the same means and variances, so we can write
\begin{align*}
	E[X_1,\dots,X_n] &= E[X_1]\dots E[X_n] \quad \text{since, the they are independent}\\
					 &= E[X_1]\dots E[X_1] \quad \text{since, the they are identically distributed}\\
					 &= E[X_1]^n
\end{align*}


\section{Sums of Random Variables}
A random variable $Y$ is given by
$$Y = X_1+\dots+X_n.$$
The linearity of expectations tells us that 
$$EY = EX_1+\dots+EX_n.$$
We can also find the variance of $Y$. 
\begin{align*}
	\textrm{Var}(X_1+X_2)=\textrm{Var}(X_1)+\textrm{Var}(X_2)+2 \textrm{Cov}(X_1,X_2).
\end{align*}
For $Y = X_1+\dots+X_n$, we can obtain a more general version of the above equation. 
\begin{align*}
	\text{Var}(X_1+X_2) &= \text{Cov} \left(\sum_{i=1}^{n}X_i, \sum_{j=1}^{n}X_j\right)\\
						&= \sum_{i=1}^{n}\sum_{j=1}^{n} \text{Cov}(X_i,X_j)\\
						&= \sum_{i=1}^{n}\text{Var}(X_i)+2\sum_{i<j}^{n}\text{Cov}(X_i, X_j).\\
\end{align*}
If the $X_i$'s are independent, then $\text{Cov}(X_i,X_j)=0$ for $i\neq j$. 

\section{Moment Generating Functions}

The $n$-th moment of a random variable $X$ is defined to be $E[X^n]$. The $n$-th central moment of $X$ is defined to be $E[(X-EX)^n]$. 

For instance, the first moment is the expected value $E[X]$. The second central moment is the variance of $X$. The moment generating function (MGF) of a random variable $X$ is a function $M_X(s)$ defined as 
$$M_X(s) = E[e^{sX}].$$
We say that MGF of $X$ exists, if there exists a positive constant $\alpha$ such that $M_X(s)$ is finite for all $s\in [-a,a]$.

\subsection{Sum of Independent Random Variables}
Suppose $X_1,\dots,X_n$ are $n$ independent random variables, and the random variable $Y$ is defined as
$$Y = X_1+\cdots + X_n.$$
Then ,
\begin{align*}
	M_Y(s) &= E[e^{sY}]\\ 
		   &= E[e^{s(X_1+\cdots + X_n)}]\\
		   &= E[e^{sX_1}e^{sX_2}\dots e^{sX_n}]\\
		   &= E[e^{sX_1}]\dots E[e^{sX_n}] \quad \text{since, they are independent}\\
		   &= M_{X_1}(s)M_{X_2}(s)\dots M_{X_n}(s)
\end{align*}

\section{Characteristic Functions}

There are some random variables for which the moment generating function does not exist on any real interval with positive length. In that case, we can use the characteristic function defined as 
$$\phi_X(\omega) = E[e^{j\omega X}],$$
where $j=\sqrt{-1}$ and $\omega$ is a real number. Note that if $X$ is a real-valued random variable, we can write 
$|e^{j\omega X}| = 1.$
Therefore, we conclude 
\begin{align*}
	|\phi_X(\omega)| &= |E\left[e^{j\omega X}\right]|\\
					 &\leq |E\left[e^{j\omega X}\right]|\\
					 &\leq 1
\end{align*}
The characteristic function has similar properties to the MGF. If $X_1,\dots,X_n$ are $n$ independent random variables, then 
$$\phi_{X_1+\dots+X_n}(\omega) = \phi_{X_1}(\omega)\dots \phi_{X_n}(\omega).$$

\section{Random Vectors}

When we have $n$ random variables, we can put them in a vector $\mathbf{X}$:
\begin{align*}
	\mathbf{X} = 
	\begin{bmatrix}
		X_1\\
		\vdots\\
		X_n\\
	\end{bmatrix}
\end{align*}
We call $\mathbf{X}$ a $n$-dimensional random vector. 

For a random vector $\mathbf{X}$, we defined the \textbf{correlation matrix}, $\mathbf{R}_{\mathbf{X}}$, as 

\begin{align*}
	\mathbf{R}_{\mathbf{X}} = E[\mathbf{X}\mathbf{X}^T] = \begin{bmatrix}
		X_1^2 & X_1X_2 & \dots & X_1X_n\\
		\vdots & \ddots & \vdots & \vdots\\
		X_nX_1 & X_nX_2 & \dots & X_n^2\\
	\end{bmatrix} = \begin{bmatrix}
		EX_1^2 & E[X_1X_2] & \dots & E[X_1X_n]\\
		\vdots & \ddots & \vdots & \vdots\\
		E[X_nX_1] & E[X_nX_2] & \dots & E[X_n^2]\\
	\end{bmatrix}
\end{align*}

The covariance matrix, $\mathbf{C}_{X}$, is defined as 
\begin{align*}
	\mathbf{C}_{\mathbf{X}} &= E[(\mathbf{X}-E\mathbf{X})(\mathbf{X}^T-E\mathbf{X})^T]\\ 
							&= \begin{bmatrix}
								(X_1-EX_1)^2 & (X_1-EX_1)(X_2-EX_2) & \dots & (X_1-EX_1)(X_n-EX_n)\\
								\vdots & \ddots & \vdots & \vdots\\
								(X_n-EX_n)(X_1-EX_1) & (X_n-EX_n)(X_2-EX_2) & \dots & (X_n-EX_n)^2\\
							\end{bmatrix}\\
							&= \begin{bmatrix}
								\text{Var}(X_1)^2 & \text{Cov}(X_1,X_2) & \dots & \text{Cov}(X_1,X_n)\\
								\vdots & \ddots & \vdots & \vdots\\
								\text{Cov}(X_n,X_1) & \text{Cov}(X_n,X_2) & \dots & \text{Var}(X_n)\\
							\end{bmatrix}\\
\end{align*}
The covariance matrix is a generalization of the variance of a random variable. 

Let $\mathbf{X}$ be an $n$-dimensional random vector and the random vector $\mathbf{Y}$ be defined as 
$$\mathbf{Y=AX+b},$$
where $\mathbf{A}$ is a fixed $m\times n$ matrix and $\mathbf{b}$ is a fixed $m$-dimensional vector. Then,
$$\mathbf{C_Y = AC_XA^T}.$$

\subsection{Properties of the Covariance Matrix}

The covariance matrix is the generalization of the variance to random vectors. It is an important matrix and is used extensively. Let's take a moment and discuss its properties. Here, we use concepts from linear algebra such as eigenvalues and positive definiteness. First note that, for any random vector $\mathbf{X}$, the covariance matrix $\mathbf{C_X}$ is a \textbf{symmetric matrix}. This is because if $\mathbf{C_X} = [c_{ij}]$, then 
\begin{align*}
	c_{ij}=\textrm{Cov}(X_i,X_j)=\textrm{Cov}(X_j,X_i)=c_{ji}.
\end{align*}
Thus, the covariance matrix has all the nice properties of symmetric matrices. In particular, $\mathbf{C_X}$ can be diagonalized and all the eigenvalues of $\mathbf{C_X}$ are real. Here, we assume $\mathbf{X}$ is a real random vector. \ie the $X_i$ can only take real values. A special property of the covariance matrix is that it is positive semi-definite (PSD). A symmetric matrix $\mathbf{M}$ is PSD if  
$$\mathbf{b^TMb}\geq 0.$$
To show that $\mathbf{C_X}$ is always PSD, let $\mathbf{b}$ be any fixed vector with $n$ elements. Define the random variable $Y$ as
$$Y = \mathbf{b^T(X-EX)}.$$
We have
\begin{align*}
	0 &\leq EY^2\\
	  &= E(YY^T)\\
	  &= \mathbf{b^T}E[\mathbf{(X-EX)}\mathbf{(X-EX)}^T]\mathbf{b}\\
	  &= \mathbf{b^T}\mathbf{C_X}\mathbf{b}
\end{align*}
Note that the eigenvalues of a PSD matrix are always larger than or equal to zero. If all the eigenvalues are strictly larger than zero, then the matrix is positive definite. From linear algebra, we know that a real symmetric matrix is positive definite if and only if all its eigenvalues are positive. 

\subsection{Functions of Random Vectors: The Method of Transformations}
A function of a random vector is a random vector. Let $\mathbf{X}$ be an $n$-dimensional random vector with joint PDF $f_{\mathbf{X}}\rvx$ and $G: \mathbb{R}^n\to \mathbb{R}^n$ be a continuous and invertible function with continuous partial derivatives and let $H = G^{-1}$. Suppose that the random vector $\mathbf{Y}$ is given by $\mathbf{Y} = G(\mathbf{X})$ and thus $\mathbf{X} = G^{-1}(\mathbf{Y}) = H(\mathbf{Y})$. That is, 
\begin{align*}
	\mathbf{X} = \begin{bmatrix}
		X_1\\
		\vdots\\
		X_n
		\end{bmatrix} = \begin{bmatrix}
		H_1(Y_1,\dots, Y_n)\\
		\vdots\\
		H_n(Y_1,\dots, Y_n)
	\end{bmatrix}
\end{align*}
Then, the PDF of $\mathbf{Y}$ is $f_{Y_1,\dots,Y_n}(y_1,\dots, y_n)$, is given by
$$f_{\mathbf{Y}}(\rvy) = f_{\mathbf{X}}(H(\rvy))|J|,$$
where $|J|$ is the Jacobian of $H$, 
\begin{align*}
	J = \text{det}\begin{bmatrix}
		\frac{\partial H_1}{\partial y_1} & \dots & \frac{\partial H_1}{\partial y_n}\\
		\vdots & \ddots & \vdots\\
		\frac{\partial H_n}{\partial y_1} & \dots & \frac{\partial H_n}{\partial y_n}\\
	\end{bmatrix}
\end{align*}
Let $\mathbf{X}$ be an $n$-dimensional random vector. Let $\mathbf{A}$ be a fixed invertible $n\times n$ matrix, and $\mathbf{b}$ be a fixed $n$-dimensional vector. A random vector $\mathbf{Y}$ is given by
$$\mathbf{Y = AX+b}.$$
The PDF of $\mathbf{Y}$ can be obtained as follows:
\begin{align*}
	\mathbf{X = A^{-1}(Y-b)}.
\end{align*}
\begin{align*}
	J = \text{det}(\mathbf{A}^{-1}) = \frac{1}{\text{det}(\mathbf{A})}.
\end{align*}
Thus, 
\begin{align*}
	f_{\mathbf{Y}}(\rvy) = \frac{1}{|\text{det}(\mathbf{A})|}f_{\mathbf{X}}(\mathbf{A}^{-1}(\rvy-\rvb))
\end{align*}


\section{Probability Bounds}

\subsection{The Union Bound and Extension}
The \textbf{union bound} or \textbf{Boole's inequality} is applicable when you need to show that the probability of union of some events is less than some value. For any two events $A$ and $B$ we have
\begin{align*}
	P(A\cup B) &= P(A)+P(B) - P(A\cap B)\\ 
			   &\leq P(A)+P(B)
\end{align*}
In general, for any events, $A_1,\dots,A_n$, we have
\begin{align*}
	P\left(\bigcup_{i=1}^nA_i\right) \leq \sum_{i=1}^{n} P(A_i).
\end{align*}

\subsection{Markov Inequality}
Let $X$ be any positive continuous random variable, we can write
\begin{align*}
	EX &= \int_{-\infty}^{\infty} x f_X(x)dx\\ 
	   &= \int_{0}^{\infty} x f_X(x)dx \quad \text{since $X$ is positive-valued}\\
	   &\geq \int_{a}^{\infty} x f_X(x)dx\\
	   &\geq \int_{a}^{\infty} a f_X(x)dx\\
	   &= a\int_{a}^{\infty} f_X(x)dx\\
	   &= a P(X\geq a).
\end{align*}
Thus, we conclude
\begin{align*}
	P(X\geq a) \leq \frac{EX}{a}, \quad \text{for any }a>0.
\end{align*}
We can prove the above inequality for discrete or mixed random variables similarly (using the generalized PDF), so we have the following result, called \textit{Markov's inequality}.

If $X$ is any non-negative random variable, then 
\begin{align*}
	P(X\geq a) \leq \frac{EX}{a}, \quad \text{for any }a>0.
\end{align*}

\subsection{Chebyshev's Inequality}
Let $X$ be any random variable. If you define $Y = (X-EX)^2$, then $Y$ is a non-negative random variable, so we can apply Markov's inequality to $Y$. In particular, for any positive real number $b$, we have
\begin{align*}
	P(X\geq b^2) \leq \frac{EX}{b^2}.
\end{align*}
Note that 
\begin{align*}
	&EY = E(X-EX)^2 = \text{Var}(X),\\
	&P(Y\geq b^2) = P\left((X-EX)^2\geq b^2\right) = P(|X-EX|\geq b).
\end{align*}
Thus, we get
\begin{align*}
	P(|X-EX|\geq b) \leq \frac{\text{Var}(X)}{b^2}.
\end{align*}

\subsection{Chernoff Bounds}
If $X$ is a random variable, then for any $a\in \mathbb{R},$ we can write
\begin{align*}
	P(X\geq a) &= P(e^{sX}\geq e^{sa}), \quad \text{for } s>0,\\
	P(X\leq a) &= P(e^{sX}\geq e^{sa}), \quad \text{for } s<0.
\end{align*}
Note that $e^{sX}$ is always a positive random variable for all $s\in \mathbb{R}$. Thus, we can apply Markov's inequality. For $s>0$, we can write
\begin{align*}
	P(X\geq a) &= P(e^{sX}\geq e^{sa})\\
			   &\leq \frac{E[e^{sX}]}{e^{sa}}.
\end{align*}
Similarly, for $s<0$, we can write
\begin{align*}
	P(X\leq a) &= P(e^{sX}\geq e^{sa})\\
			   &\leq \frac{E[e^{sX}]}{e^{sa}}.
\end{align*}
Also note that $E[e^{sX}]$ is the moment generating function, $M_X(s)$. Thus, we conclude 
\begin{align*}
	P(X\geq a) \leq e^{-sa}M_X(s), \quad \text{for all } s>0,\\
	P(X\leq a) \leq e^{-sa}M_X(s), \quad \text{for all } s<0.
\end{align*}
Since Chernoff bounds are valid for all values of $s>0$ and $s<0$, we can choose $s$ in a way to obtain the best bound, that is 
\begin{align*}
	P(X\geq a) \leq \min_{s>0} e^{-sa}M_X(s)\\
	P(X\leq a) \leq \min_{s<0} e^{-sa}M_X(s)
\end{align*}

\paragraph{Comparison between Markov, Chebyshev, and Chernoff Bounds:} For a random varible $X\sim \text{Binom}(n,p)$, upper bounds of $P(X\geq \a n)$ of each bound when $p=\frac{1}{4}$ and $\alpha=\frac{3}{4}$ is given by 
\begin{align*}
	P(X\geq \frac{3n}{4})&\leq \frac{2}{3}, \quad\text{Markov}\\
	P(X\geq \frac{3n}{4})&\leq \frac{4}{n}, \quad\text{Chebyshev}\\
	P(X\geq \frac{3n}{4})&\leq \frac{16}{27}^{\frac{n}{4}}, \quad\text{Chernoff}.
\end{align*}
The bound given by Markov is the weakest one. It is constant and does not change as $n$ increases. The bound given by Chebyshev's inequality is stronger than the one given by Markov's inequality. The strongest bound is the Chernoff bound since it goes to zero exponentially.


\subsection{Cauchy-Schwarz Inequality}

For any two random variables $X$ and $Y$, we have
\begin{align*}
	|EXY|\leq \sqrt{E[X^2]E[Y^2]},
\end{align*}
where equality holds if and only if $X=\alpha Y$, for some constant $\alpha\in \mathbb{R}$.

\subsection{Jensen's Inequality}
