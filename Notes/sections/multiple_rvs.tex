\chapter{Multiple Random Variables}

\section{Joint Distributions and Independence}

For three or more random variables, the joint PDF, joint PMF, and joint CDF are defined in a similar way to wat we have already seen for the case of two random variables. Let $X_1,\dots,Xn$ be $n$ discrete random variables. The joint PMF of $X_1,\dots,X_n$ is defined as
$$P_{X_1,\dots X_n}(x_1,\dots, x_n) = P(X_1=x_1,\dots,X_n=x_n).$$

For $n$ jointly continuous random variables $X_1,\dots,Xn$ the joint PDF is defined to be the function $f_{X_1,\dots,Xn}(x_1,\dots,x_n)$ such that the probability of any set $A\subset \mathbb{R}^n $ is given by the integral of the PDF over the set $A$. In particular, for a set $A\in \mathbb{R}^n$, we can write 
$$P\left( (X_1,\dots X_n)\in A \right)\int \cdots \int_A \cdots \int f_{X_1,\dots,X_n}(x_1,\dots, x_n) dx_1,\dots,dx_n.$$

The marginal PDF of $X_i$ can be obtained by integrating all other $X_j$'s. For example,
$$\int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} f_{X_1,\dots,X_n}(x_1,\dots, x_n) dx_1,\dots,dx_n.$$

The joint CDF of $n$ random variables $X_1,\dots, X_n$ is defined as
$$F_{X_1,\dots,X_n}(x_1,\dots, x_n) = P(X_1\leq x_1, \dots, X_n\leq x_n).$$

\paragraph{Independence} The idea of Independence is exactly the same as what we have seen before:
\begin{itemize}
	\item $F_{X_1,\dots,X_n}(x_1,\dots, x_n) = F_{X_1}(x_1)F_{X_2}(x_2)\cdots F_{X_n}(x_n).$
	\item Equivalently, if $X_1,\dots, X_n$ are discrete, then they are independent if for all 
		$$P_{X_1,\dots,X_n}(x_1,\dots, x_n) = P_{X_1}(x_1)P_{X_2}(x_2)\cdots P_{X_n}(x_n).$$
	\item If $X_1,\dots, X_n$ are continuous, then they are independent if for all 
		$$f_{X_1,\dots,X_n}(x_1,\dots, x_n) = f_{X_1}(x_1)f_{X_2}(x_2)\cdots f_{X_n}(x_n).$$
	\item If random variables are independent, 
		$$E[X_1,\dots,X_n] = E[X_1]\dots E[X_n]. $$
\end{itemize}

If random variables $X_1,\dots,X_n$ are independent and identically distributed (i.i.d.) then they will have the same means and variances, so we can write
\begin{align*}
	E[X_1,\dots,X_n] &= E[X_1]\dots E[X_n] \quad \text{since, the they are independent}\\
					 &= E[X_1]\dots E[X_1] \quad \text{since, the they are identically distributed}\\
					 &= E[X_1]^n
\end{align*}


\section{Sums of Random Variables}
A random variable $Y$ is given by
$$Y = X_1+\dots+X_n.$$
The linearity of expectations tells us that 
$$EY = EX_1+\dots+EX_n.$$
We can also find the variance of $Y$. 
\begin{align*}
	\textrm{Var}(X_1+X_2)=\textrm{Var}(X_1)+\textrm{Var}(X_2)+2 \textrm{Cov}(X_1,X_2).
\end{align*}
For $Y = X_1+\dots+X_n$, we can obtain a more general version of the above equation. 
\begin{align*}
	\text{Var}(X_1+X_2) &= \text{Cov} \left(\sum_{i=1}^{n}X_i, \sum_{j=1}^{n}X_j\right)\\
						&= \sum_{i=1}^{n}\sum_{j=1}^{n} \text{Cov}(X_i,X_j)\\
						&= \sum_{i=1}^{n}\text{Var}(X_i)+2\sum_{i<j}^{n}\text{Cov}(X_i, X_j).\\
\end{align*}
If the $X_i$'s are independent, then $\text{Cov}(X_i,X_j)=0$ for $i\neq j$. 

\section{Moment Generating Functions}

The $n$-th moment of a random variable $X$ is defined to be $E[X^n]$. The $n$-th central moment of $X$ is defined to be $E[(X-EX)^n]$. 

For instance, the first moment is the expected value $E[X]$. The second central moment is the variance of $X$. The moment generating function (MGF) of a random variable $X$ is a function $M_X(s)$ defined as 
$$M_X(s) = E[e^{sX}].$$
We say that MGF of $X$ exists, if there exists a positive constant $\alpha$ such that $M_X(s)$ is finite for all $s\in [-a,a]$.

\subsection{Sum of Independent Random Variables}
Suppose $X_1,\dots,X_n$ are $n$ independent random variables, and the random variable $Y$ is defined as
$$Y = X_1+\cdots + X_n.$$
Then ,
\begin{align*}
	M_Y(s) &= E[e^{sY}]\\ 
		   &= E[e^{s(X_1+\cdots + X_n)}]\\
		   &= E[e^{sX_1}e^{sX_2}\dots e^{sX_n}]\\
		   &= E[e^{sX_1}]\dots E[e^{sX_n}] \quad \text{since, they are independent}\\
		   &= M_{X_1}(s)M_{X_2}(s)\dots M_{X_n}(s)
\end{align*}







