\chapter{Multiple Random Variables}

\section{Joint Distributions and Independence}

For three or more random variables, the joint PDF, joint PMF, and joint CDF are defined in a similar way to wat we have already seen for the case of two random variables. Let $X_1,\dots,Xn$ be $n$ discrete random variables. The joint PMF of $X_1,\dots,X_n$ is defined as
$$P_{X_1,\dots X_n}(x_1,\dots, x_n) = P(X_1=x_1,\dots,X_n=x_n).$$

For $n$ jointly continuous random variables $X_1,\dots,Xn$ the joint PDF is defined to be the function $f_{X_1,\dots,Xn}(x_1,\dots,x_n)$ such that the probability of any set $A\subset \mathbb{R}^n $ is given by the integral of the PDF over the set $A$. In particular, for a set $A\in \mathbb{R}^n$, we can write 
$$P\left( (X_1,\dots X_n)\in A \right)\int \cdots \int_A \cdots \int f_{X_1,\dots,X_n}(x_1,\dots, x_n) dx_1,\dots,dx_n.$$

The marginal PDF of $X_i$ can be obtained by integrating all other $X_j$'s. For example,
$$\int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} f_{X_1,\dots,X_n}(x_1,\dots, x_n) dx_1,\dots,dx_n.$$

The joint CDF of $n$ random variables $X_1,\dots, X_n$ is defined as
$$F_{X_1,\dots,X_n}(x_1,\dots, x_n) = P(X_1\leq x_1, \dots, X_n\leq x_n).$$

\paragraph{Independence} The idea of Independence is exactly the same as what we have seen before:
\begin{itemize}
	\item $F_{X_1,\dots,X_n}(x_1,\dots, x_n) = F_{X_1}(x_1)F_{X_2}(x_2)\cdots F_{X_n}(x_n).$
	\item Equivalently, if $X_1,\dots, X_n$ are discrete, then they are independent if for all 
		$$P_{X_1,\dots,X_n}(x_1,\dots, x_n) = P_{X_1}(x_1)P_{X_2}(x_2)\cdots P_{X_n}(x_n).$$
	\item If $X_1,\dots, X_n$ are continuous, then they are independent if for all 
		$$f_{X_1,\dots,X_n}(x_1,\dots, x_n) = f_{X_1}(x_1)f_{X_2}(x_2)\cdots f_{X_n}(x_n).$$
	\item If random variables are independent, 
		$$E[X_1,\dots,X_n] = E[X_1]\dots E[X_n]. $$
\end{itemize}

If random variables $X_1,\dots,X_n$ are independent and identically distributed (i.i.d.) then they will have the same means and variances, so we can write
\begin{align*}
	E[X_1,\dots,X_n] &= E[X_1]\dots E[X_n] \quad \text{since, the they are independent}\\
					 &= E[X_1]\dots E[X_1] \quad \text{since, the they are identically distributed}\\
					 &= E[X_1]^n
\end{align*}


\section{Sums of Random Variables}
A random variable $Y$ is given by
$$Y = X_1+\dots+X_n.$$
The linearity of expectations tells us that 
$$EY = EX_1+\dots+EX_n.$$
We can also find the variance of $Y$. 
\begin{align*}
	\textrm{Var}(X_1+X_2)=\textrm{Var}(X_1)+\textrm{Var}(X_2)+2 \textrm{Cov}(X_1,X_2).
\end{align*}
For $Y = X_1+\dots+X_n$, we can obtain a more general version of the above equation. 
\begin{align*}
	\text{Var}(X_1+X_2) &= \text{Cov} \left(\sum_{i=1}^{n}X_i, \sum_{j=1}^{n}X_j\right)\\
						&= \sum_{i=1}^{n}\sum_{j=1}^{n} \text{Cov}(X_i,X_j)\\
						&= \sum_{i=1}^{n}\text{Var}(X_i)+2\sum_{i<j}^{n}\text{Cov}(X_i, X_j).\\
\end{align*}
If the $X_i$'s are independent, then $\text{Cov}(X_i,X_j)=0$ for $i\neq j$. 

\section{Moment Generating Functions}

The $n$-th moment of a random variable $X$ is defined to be $E[X^n]$. The $n$-th central moment of $X$ is defined to be $E[(X-EX)^n]$. 

For instance, the first moment is the expected value $E[X]$. The second central moment is the variance of $X$. The moment generating function (MGF) of a random variable $X$ is a function $M_X(s)$ defined as 
$$M_X(s) = E[e^{sX}].$$
We say that MGF of $X$ exists, if there exists a positive constant $\alpha$ such that $M_X(s)$ is finite for all $s\in [-a,a]$.

\subsection{Sum of Independent Random Variables}
Suppose $X_1,\dots,X_n$ are $n$ independent random variables, and the random variable $Y$ is defined as
$$Y = X_1+\cdots + X_n.$$
Then ,
\begin{align*}
	M_Y(s) &= E[e^{sY}]\\ 
		   &= E[e^{s(X_1+\cdots + X_n)}]\\
		   &= E[e^{sX_1}e^{sX_2}\dots e^{sX_n}]\\
		   &= E[e^{sX_1}]\dots E[e^{sX_n}] \quad \text{since, they are independent}\\
		   &= M_{X_1}(s)M_{X_2}(s)\dots M_{X_n}(s)
\end{align*}

\section{Characteristic Functions}

There are some random variables for which the moment generating function does not exist on any real interval with positive length. In that case, we can use the characteristic function defined as 
$$\phi_X(\omega) = E[e^{j\omega X}],$$
where $j=\sqrt{-1}$ and $\omega$ is a real number. Note that if $X$ is a real-valued random variable, we can write 
$|e^{j\omega X}| = 1.$
Therefore, we conclude 
\begin{align*}
	|\phi_X(\omega)| &= |E\left[e^{j\omega X}\right]|\\
					 &\leq |E\left[e^{j\omega X}\right]|\\
					 &\leq 1
\end{align*}
The characteristic function has similar properties to the MGF. If $X_1,\dots,X_n$ are $n$ independent random variables, then 
$$\phi_{X_1+\dots+X_n}(\omega) = \phi_{X_1}(\omega)\dots \phi_{X_n}(\omega).$$

\section{Random Vectors}

When we have $n$ random variables, we can put them in a vector $\mathbf{X}$:
\begin{align*}
	\mathbf{X} = 
	\begin{bmatrix}
		X_1\\
		\vdots\\
		X_n\\
	\end{bmatrix}
\end{align*}
We call $\mathbf{X}$ a $n$-dimensional random vector. 

For a random vector $\mathbf{X}$, we defined the \textbf{correlation matrix}, $\mathbf{R}_{\mathbf{X}}$, as 

\begin{align*}
	\mathbf{R}_{\mathbf{X}} = E[\mathbf{X}\mathbf{X}^T] = \begin{bmatrix}
		X_1^2 & X_1X_2 & \dots & X_1X_n\\
		\vdots & \ddots & \vdots & \vdots\\
		X_nX_1 & X_nX_2 & \dots & X_n^2\\
	\end{bmatrix} = \begin{bmatrix}
		EX_1^2 & E[X_1X_2] & \dots & E[X_1X_n]\\
		\vdots & \ddots & \vdots & \vdots\\
		E[X_nX_1] & E[X_nX_2] & \dots & E[X_n^2]\\
	\end{bmatrix}
\end{align*}

The covariance matrix, $\mathbf{C}_{X}$, is defined as 
\begin{align*}
	\mathbf{C}_{\mathbf{X}} &= E[(\mathbf{X}-E\mathbf{X})(\mathbf{X}^T-E\mathbf{X})^T]\\ 
							&= \begin{bmatrix}
								(X_1-EX_1)^2 & (X_1-EX_1)(X_2-EX_2) & \dots & (X_1-EX_1)(X_n-EX_n)\\
								\vdots & \ddots & \vdots & \vdots\\
								(X_n-EX_n)(X_1-EX_1) & (X_n-EX_n)(X_2-EX_2) & \dots & (X_n-EX_n)^2\\
							\end{bmatrix}\\
							&= \begin{bmatrix}
								\text{Var}(X_1)^2 & \text{Cov}(X_1,X_2) & \dots & \text{Cov}(X_1,X_n)\\
								\vdots & \ddots & \vdots & \vdots\\
								\text{Cov}(X_n,X_1) & \text{Cov}(X_n,X_2) & \dots & \text{Var}(X_n)\\
							\end{bmatrix}\\
\end{align*}
The covariance matrix is a generalization of the variance of a random variable. 

Let $\mathbf{X}$ be an $n$-dimensional random vector and the random vector $\mathbf{Y}$ be defined as 
$$\mathbf{Y=AX+b},$$
where $\mathbf{A}$ is a fixed $m\times n$ matrix and $\mathbf{b}$ is a fixed $m$-dimensional vector. Then,
$$\mathbf{C_Y = AC_XA^T}.$$

\subsection{Properties of the Covariance Matrix}

The covariance matrix is the generalization of the variance to random vectors. It is an important matrix and is used extensively. Let's take a moment and discuss its properties. Here, we use concepts from linear algebra such as eigenvalues and positive definiteness. First note that, for any random vector $\mathbf{X}$, the covariance matrix $\mathbf{C_X}$ is a \textbf{symmetric matrix}. This is because if $\mathbf{C_X} = [c_{ij}]$, then 
\begin{align*}
	c_{ij}=\textrm{Cov}(X_i,X_j)=\textrm{Cov}(X_j,X_i)=c_{ji}.
\end{align*}

