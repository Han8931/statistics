\chapter{Hypothesis Testing}

A \textit{hypothesis} is a statement that requires testing by observation to determine whether it is true or false. A few examples:
\begin{itemize}
	\item The coin is unbiased.
	\item Students entering the graduate program have GPA ≥3.
\end{itemize}
As you can see from these examples, a hypothesis is something we can test based on the data. Therefore, being correct or wrong depends on the statistics we have and the cutoff threshold. \textbf{Accepting or rejecting a hypothesis does not mean that the statement is correct or wrong, since the truth is unknown. If we accept a hypothesis, we have made a better decision solely based on the statistical evidence}. It is possible that tomorrow when you have collected more data we may reject a previously accepted hypothesis.

The procedure for testing whether a hypothesis should be accepted or rejected is known as \textit{hypothesis testing}. In hypothesis testing, we often have two opposite hypotheses:
\begin{itemize}
	\item The hypothesis $H_0$ is called the \textit{null hypothesis} (default hypothesis)
		\begin{itemize}
			\item Assumed to be true
		\end{itemize}
	\item The hypothesis $H_1$ is called the \textit{alternative hypothesis} (contradictory hypothesis).  
\end{itemize}

\begin{commentbox}{Example}
Suggest a null hypothesis and an alternative hypothesis regarding whether a coin is unbiased. 

Solution: Let $\theta$ be the probability of getting a head.
\begin{itemize}
	\item $H_0: \theta = 0.5, $ and $ H_1: \theta > 0.5$. This is a \textbf{one-sided} alternative.
	\item $H_0: \theta = 0.5, $ and $ H_1: \theta < 0.5$. This is \textbf{another one-sided} alternative.
	\item $H_0: \theta = 0.5, $ and $ H_1: \theta ̸\neq 0.5$. This is a \textbf{two-sided} alternative.
\end{itemize}
\end{commentbox}


\section{Critical-value test}
In hypothesis testing, there are two major approaches: 
\begin{enumerate}
	\item \textbf{critical-value test}
	\item \textbf{p-value test}. 
\end{enumerate}

Suppose that we have a 4-sided die and our goal is to test whether the die is unbiased. To do so, we define the null and the alternative hypotheses as
\begin{itemize}
	\item $H_0: \theta = 0.25$, which is our default belief.
	\item $H_1: \theta > 0.25$, which is a one-sided alternative.
\end{itemize}
We must obtain data prior to conducting any hypothesis testing. Let's assume that we have thrown the die $N = 1000$ times. We find that ``3'' appears 290 times (we could just as well have chosen 1, 2, or 4). We let $X_1, \dots , X_1000$ be the $N = 1000$ binary random variables representing whether we have obtained a ``3'' or not. If the true probability is $\theta = 0.25$, then we will have $P[X_n = 3] = \theta = 0.25$ and $P[X_n ̸\neq 3] = 1 -\theta = 0.75$. We know that we cannot access the true probability, so we can only construct an estimator of the probability:

\begin{align*}
	\hat{\Theta} = \frac{1}{N}\sum_{n=1}^N X_n.
\end{align*}
In this example, we can show that $\hat{\Theta} = 290/1000 = 0.29$.

To make our problem slightly easier, we pretend that we know the variance $Var[X_n]$. In practice, we certainly do not know $Var[X_n]$, and so we need to estimate the variance. If we knew the variance, it should be $Var[X_n] = \theta(1-\theta) = 0.25(1 -0.25) = 0.1875$, because $X_n$ is a Bernoulli random variable with a mean $\theta$.

The question asked by hypothesis testing is: How far is ``$\hat{\Theta} = 0.29$'' from ``\theta = 0.25''? 

\begin{itemize}
	\item If the statistic generated by our data, $\hat{\Theta} = 0.29$, is ``far'' from the hypothesized $\theta = 0.25$, then we need to reject $H_0$ because $H_0$ says that $\theta = 0.25$. 
	\item However, if there is no strong evidence that $\theta > 0.25$, we will need to assume that $H_0$ may possibly be true. So the key question is what is meant by ``far''.
\end{itemize}

For many problems like this one, it is possible to analyze the PDF of $\hat{\Theta}$. Since $\hat{\Theta}$ is the sample average of a sequence of Bernoulli random variables, it follows that $\hat{\Theta}$ is a binomial (with a scaling constant $1/N$). If $N$ is large enough, e.g., $N \geq 30$, the Central Limit Theorem tells us that $\hat{\Theta}$ is also very close to a Gaussian. Therefore, we can more or less claim that
\begin{align*}
	\hat{\Theta}\sim \mathcal{N}\left(\theta, \frac{\sigma^2}{N}\right).
\end{align*}
With a simple translation and scaling, we can normalize $\hat{\Theta}$ to obtain $\hat{Z}$:
\begin{align*}
	\hat{Z} = \frac{\hat{\Theta}-\theta}{\sigma/\sqrt{N}}\sim \mathcal{N}\left(0, 1\right).
\end{align*}

One essential element of hypothesis testing is the \textit{cutoff threshold}, which is defined through the \textit{critical level} $\alpha$. It is the area under the curve of the PDF of $\hat{Z}$. Typically, $\alpha$ is chosen to be a small value, such as $\alpha = 0.05$ (corresponding to a 5\% margin). The corresponding cutoff is known as the \textit{critical value} $z_\alpha$, which is defined as
\begin{center}
	$z_\alpha$: cutoff location where area under the curve is $\alpha$	.
\end{center}
If $\hat{Z}$ is $\mathcal{N}(0,1)$ and if we are looking at the right-hand tail, it follows that
\begin{align*}
	z_\alpha = \Phi^{-1}(1-\alpha).
\end{align*}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{./images/cutoff_value.png}
	\caption{There are two axes: the $\hat{\Theta}$-axis (which is the estimator) and the $\hat{Z}$-axis (which is the normalized variable). The values corresponding to each axis are shown in the figure. For example. $\hat{\Theta} = 0.29$ is equivalent to $\hat{Z} = 2.92$, and $\hat{\Theta} = 0.25$ is equivalent to $\hat{Z} = 0$, etc. Therefore, when we ask how far ``$\hat{\Theta} = 0.29$'' is from ``$\theta = 0.25$'', we can map this question from the $\hat{\Theta}$-axis to the $\hat{Z}$-axis, and ask the relative position of $\hat{Z}$ from the origin.}
\end{figure}


\begin{lstlisting}[language=Python]
# Etimate the Z_hat value
import numpy as np
Theta_hat = 0.29 # Your estimate
theta     = 0.25 # Your hypothesis
N         = 1000 # Number of samples
sigma     = np.sqrt(theta*(1-theta)) # Known standard deviation

Z_hat = (Theta_hat - theta)/(sigma / np.sqrt(N))
print(Z_hat)
\end{lstlisting}

To determine the critical value $z_\alpha$, 
\begin{lstlisting}[language=Python]
# Compute the critical value
import scipy.stats as stats
alpha = 0.05
z_alpha = stats.norm.ppf(1-alpha, 0, 1)
\end{lstlisting}
\begin{itemize}
	\item PPF stands for \textit{percent point function}, which is an inverse of CDF.
\end{itemize}

Do we have enough evidence to reject $H_0$ in this example? Yes!
\begin{itemize}
	\item The estimated value $\hat{\Theta} = 0.29$ is equivalent to $\hat{Z} = 2.92$, which is much too far from the cutoff $z_\alpha = 1.65$. 
	\item In other words, we conclude that at a 5\% critical level we have strong evidence to believe that the die is biased. Therefore, we need to reject $H_0$.
\end{itemize}

The decision based on comparing the critical value is known as the \textit{critical-value test}.
\begin{enumerate}
	\item Set a critical value $z_\alpha$. Then, compute 
		$$\hat{Z} = \frac{\hat{\Theta}-\theta}{\sigma/\sqrt{N}}$$
	\item If $\hat{Z}\geq z_\alpha$, then reject $H_0$.
	\item If $\hat{Z}< z_\alpha$, then keep $H_0$.
\end{enumerate}

The critical-value test belongs to a larger family of testing procedures based on decision theory. To give you a preview of the general theory of hypothesis testing, we define a \textit{decision rule}, a function that maps a realization of the estimator to a binary decision space. In our problem the estimator is $\hat{Z}$ (or equivalently $\hat{\Theta}$). We denote its realization by $\hat{z}$. The binary decision space is $\{H_0, H_1\}$, corresponding to whether we want to claim $H_0$ or $H_1$. Claiming $H_0$ is equivalent to keeping $H_0$, and claiming $H_1$ is equivalent to rejecting $H_0$. For the critical-value test, the decision rule $\delta(\cdot) : \mathbb{R} \to \left\{0, 1\right\}$ is given by the equation (for testing a right-hand tail):


\begin{align*}
	\delta(\hat{z}) = \begin{cases}
		1&\text{ if } \hat{z}\geq z_\alpha \text{ claim }H_1.\\
		0&\text{ if } \hat{z}< z_\alpha \text{ claim }H_0.
	\end{cases}
\end{align*}

\section{$p$-value test}

An alternative to the critical-value test is the \textit{p-value test}. Instead of looking at the cutoff value $z_α$, \textbf{we inspect the probability of obtaining our observation if $H_0$ is true}. In other words, it is the probability of getting data at least this extreme, if $H_0$ were true.

In sum, a decision can be made as follows:
\begin{itemize}
	\item If $p$-value $< \alpha$, the result is unlikely under $H_0$. This implies that it is ``statistically significant'', so we reject $H_0$.
	\item If $p$-value $\ge \alpha$, The result is not that surprising, so we do not reject$H_0$ 
\end{itemize}

Let's consider a toy problem. Suppose that we have two hypotheses about flipping a coin:
\begin{itemize}
	\item $H_0: \theta = 0.9$, which is our default belief.
	\item $H_1: \theta < 0.9$, which is a one-sided alternative.
\end{itemize}
It was found that with $N = 150$ coin flips, the coin landed on heads 128 times. Thus the estimator is $\hat{\Theta} = \frac{128}{150}=0.853$. Then, by following our previous procedures, we have that
\begin{align*}
	\hat{Z} = \frac{\hat{\Theta}-\theta}{\sigma/\sqrt{N}} = \frac{0.853-0.9}{\sqrt{\frac{0.9(1-0.9)}{150}}} = -1.92.
\end{align*}
At this point we can follow the previous subsection by computing the critical value zα and make the decision. However, let's take a different route. We want to know what is the probability under the curve if we integrate the PDF of $\hat{Z}$ from $-\infty$ to $-1.92$. This is easy. Since $\hat{z}$ is $\mathcal{N}(0, 1)$, it follows from the CDF of a Gaussian that
\begin{align*}
	\underbrace{\mathbb{P}[\hat{Z}\leq -1.92]}_{p\text{-value}} = 0.0274.
\end{align*}
The value 0.0274 is the pink area under the curve, which is the PDF of $\hat{Z}$. Since the area under the curve is less than the critical level $\alpha$ (say 5\%), we reject the null hypothesis.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{./images/p-value.png}
	\caption{ The p-value test asks us to look at the probability of  $\hat{Z}\leq \hat{z}$. If this probability (the p-value) is less than the critical level $\alpha$, we have significant evidence to reject the null hypothesis.}
\end{figure}

\begin{lstlisting}[language=Python]
# Compute the p-value
import scipy.stats as stats
p = stats.norm.cdf(-1.92,0,1)
\end{lstlisting}

In this example, the probability $P[\hat{Z}\leq -1.92]$ is known as the $p$-value. It is the probability of $\hat{Z}\leq \hat{z}$, under the distribution mandated by the null hypothesis, where $z$ is the (normalized) estimated value based on data. Using our example, $z is -1.92$. By ``distribution mandated by the null hypothesis'' we mean that the PDF of $\hat{z}$ is the PDF that the null hypothesis wants. In the above example the PDF is $Gaussian(0, 1)$, corresponding to $Gaussian(\theta, \sigma / \sqrt{N})$ for $\hat{\Theta}$.

The decision rule based on the $p$-value is
\begin{align*}
	\delta(\hat{z}) = \begin{cases}
		1&\text{ if } \mathbb{P}[\hat{Z}\leq \hat{z}]< \alpha \text{ claim }H_1.\\
		0&\text{ if } \mathbb{P}[\hat{Z}\leq \hat{z}]\geq \alpha \text{ claim }H_0.
	\end{cases}
\end{align*}
If the alternative hypothesis is right-handed, then the probability becomes $\mathbb{P}[\hat{Z}\geq \hat{z}]$ instead.


\begin{commentbox}{Example}
We flip a coin for $N = 150$ times and find that 128 are heads. Consider two hypotheses
\begin{itemize}
	\item $H_0: \theta = 0.9$, which is our default belief. 
	\item $H_1: \theta \neq 0.9$, which is a two-sided alternative.
\end{itemize}
For a critical level of $\alpha = 0.05$, shall we keep or reject $H_0$?

 We know that $\hat{\Theta}=128/150 = 0.853$. The normalized statistic is
 \begin{align*}
	\hat{Z} = \frac{\hat{\Theta}-\theta}{\sigma/\sqrt{N}} = \frac{0.853-0.9}{\sqrt{\frac{0.9(1-0.9)}{150}}} = -1.92.
 \end{align*}
To compute the $p$-value, we observe that the two-sided test means that we consider the two tails. Thus, we have
\begin{align*}
	p-\text{value} &= \mathbb{P}[|\hat{Z}|\geq 1.92]\\
				   &= 2\times \mathbb{P}[\hat{Z}\geq 1.92]\\
				   &= 2\times 0.0274 = 0.055.
\end{align*}
For a critical level of $\alpha = 0.05$, the $p$-value is larger. This means that the probability of obtaining $|Z| > 1.92$ is not extreme enough. Therefore, we do not have sufficient evidence to reject the null hypothesis.

If we take the critical-value test, we will reach the same conclusion. The critical value for $\alpha = 0.05$ is determined by taking the inverse CDF at $1-0.025$, giving
\begin{align*}
	z_\alpha = \Phi^{-1}(1-\frac{\alpha}{2})=1.96.
\end{align*}

\centering
\includegraphics[scale=0.5]{./images/two_sided.png}

\end{commentbox}

\subsection{$p$-value hacking}

\textit{$p$-value hacking}, also known as \textit{data dredging}, data fishing, data snooping or data butchery, is \textbf{an exploitation of data analysis in order to discover patterns which would be presented as statistically significant, when in reality, there is no underlying effect}. In other words, $p$-hacking is \textbf{running statistical tests on a set of data until some statistically significant results arise}. That can be done in a few different ways, for example: by stopping the collection of data once you get a $p<0.05$ (reject null hypothesis to claim their hypothesis), analyzing many outcomes, but only reporting those with $p<0.05$, using covariates, excluding participants, etc. 


\section{$Z$-test and $T$-test}

The critical-value test and the $p$-value tests are generic tools for hypothesis testing. In this subsection we introduce the $Z$-test and the $T$-test. It is important to understand that the $Z$-test and the $T$-test refer to the distributional assumptions we make about the variance. They define the distribution we use to conduct the test but not the tools. In fact, both the $Z$-test and the $T$-test can be implemented using the critical-value test or the $p$-value test.

The difference between the Gaussian distribution and the T distribution is mainly attributable to the knowledge about \textbf{the population variance}: 
\begin{itemize}
	\item If the variance is known, the distribution of the estimator (which in our case is the sample average) is \textbf{Gaussian}. 
	\item If the variance is estimated from the sample, the distribution of the estimator will follow a Student's \textbf{t-distribution}.
\end{itemize}

\section{Neyman-Pearson Test}

\subsection{Null and alternative distributions}

When we discussed hypothesis testing in the previous section, we focused exclusively on the null hypothesis $H_0$. Regardless of whether we are studying the Z-test or the T-test, using the critical value or the $p$-value, all the distributions are associated with the distribution under $H_0$.

What do we mean by ``distribution under $H_0$''? Using $\hat{\Theta}$ as an example, the PDF of $\hat{\Theta}$ is assumed to be $\mathcal{N}(\theta, \sigma^2/N)$. This Gaussian, centered at $\theta$, is the distribution assumed under $H_0$. As we decide whether to keep or reject $H_0$, we look at the critical value and the $p$-value of the test statistic under $\mathcal{N}(\theta, \sigma^2/N)$. Importantly, the analysis of hypothesis testing is not just about $H_0$ - it is also about the alternative hypothesis $H_1$, which uses a different PDF. For example, $H_1$ could use $\mathcal{N}(\theta', \sigma^2/N)$ for $\theta' > \theta$. Therefore, for the same testing statistic $\hat{\Theta}$, we can check how close it is to $H_1$.

To capture both distributions, we define
\begin{itemize}
	\item $f_0(y) = f_Y(y|H_0)$
	\item $f_1(y) = f_Y(y|H_1)$
\end{itemize}
The first PDF defines the distribution when the true model is $H_0$. The second PDF is the distribution when the true model is $H_1$.


\begin{align*}
	\delta(y) = \begin{cases}
		1&\text{ if } y\in R_\alpha \text{ reject }H_1.\\
		0&\text{ if } y\notin R_\alpha \text{ keep }H_0.
	\end{cases}
\end{align*}


\begin{commentbox}{Example}
Consider $H_0 : \theta = 0.35$ and $H_1 : \theta > 0.35$. It was found that the sample average over 1009 samples is $\hat{\Theta}= 0.387$, with $\sigma^2 = 0.227$. The normalized test statistic is $\hat{Z} = \sqrt{N}( \hat{\Theta}-\theta)/\sigma = 2.432$. At a 5\% critical level, define the decision rule based on the critical-value approach.

If $\alpha = 0.05$, it follows that $z_\alpha = \Phi^{-1}(1-0.05) = 1.65$, which is bigger than 1.65, so we reject $H_0$.
\end{commentbox}

\subsection{Type 1 and type 2 errors}

\begin{itemize}
	\item FPR: Type 1 error (declare $H_1$, truth is $H_0$), the number of FP divided by the total number of cases.
	\item FNR: Type 2 error (declare $H_0$, truth is $H_1$), the number of FN divided by the total number of cases. 
\end{itemize}
The center is the $H_1$, so the positive means you declare the $H_1$, and the negative is the case, where you reject $H_1$.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.8]{./images/error_types.png}
	\caption{The null hypothesis says that the test result says you have no disease and the alternative hypothesis says you have disease. Then, you set a threshold at the line. Type 1 error is the case where you are diagnosed with positive, but actually you don't have the disease.}
\end{figure}

\section{ROC and Precision-Recall Curve}
Being a binary decision rule, the hypothesis testing procedure shares many similarities with a two-class classification algorithm. Given a testing statistic or a testing sample, both the hypothesis testing and a classification algorithm will report YES or NO. Therefore, any performance evaluation metric developed for hypothesis testing is equally applicable to classification and vice versa. 

The topic we study in this section is the \textbf{receiver operating characteristic} (ROC) curve and the \textbf{precision-recall} (PR) curve. The ROC curve and the PR curve are arguably the most popular metrics in modern machine learning, in particular for classification, detection, and segmentation tasks in computer vision. There are many unresolved questions about these two curves and there are many debates about how to use them. Our goal is not to add another voice to the debate; rather, we would like to fill in the gap between the hypothesis testing theory (particularly the Neyman-Pearson framework) and these two sets of curves. We will establish the equivalence between the two curves and leave the open-ended debates to you.

\subsection{Receiver Operating Characteristic (ROC)}








% We need to design a test to whether we accept the null hypothesis or not. We first toss the coin 100 times and count the number of heads. Let $X$ be the number of heads that we observe, 
% $$X\sim Bin(100, \theta).$$
% If $H_0$ is true, then $\theta = \theta_0 = 1/2$, so we expect the number of heads to be close to 50. Thus, intuitively we can say that if we observe close to 50 heads, we should accept $H_0$, otherwise we should reject it. More specifically, we suggest the following criteria: if $|X-50|$ is less than or equal to some threshold, we accept $H_0$. On the other hand, if $|X-50|$ is larger than the threshold, we reject $H_0$ and accept $H_1$:  
% \begin{itemize}
% 	\item If $|X-50|\leq t$, accept $H_0$.
% 	\item If $|X-50|> t$, accept $H_1$.
% \end{itemize}
% How do we determine the threshold $t$? We need to state some requirements for our test. An important factor is \textbf{the probability of error}. One of the sources of the error is from when we wrongly reject $H_0$ when it is true. We call this \textit{type 1 error}. This is a case where the number of observed heads is larger than $t$, \ie  $|X-50|> t$ when $H_0$ is true. 
% $$P(\text{type 1 error}) = P(|X-50|> t | H_0).$$
% We read this as the probability of making type 1 error when $H_0$ is true. Note that this is not a conditional probability, since in classical statistics, we do not treat $H_0$ and $H_1$ as random events.

% We can choose $t$ as follows:
% $$P(\text{type 1 error}) \leq \alpha = 0.05.$$
% Here $\alpha$ is called \textit{the level of significance} and we say the test has a \textit{significance level} $\alpha$. By CLT, we can approximate the binomial distribution when $n$ is large. Thus, 
% \begin{align*}
% 	Y=\frac{X-n\theta_0}{\sqrt{n\theta_0(1-\theta_0)}}=\frac{X-50}{5}, 
% \end{align*}
% where $n=100$ and $\theta_0=1/2$. This is approximately a standard normal random variable. To exploit the CLT, we can divide the both sides of the inequality as follows: 
% \begin{align*}
% 	P(\text{type 1 error}) &=  P(\left|\frac{X-50}{5}\right|> \frac{t}{5} | H_0)\\
% 						   &= P(|Y|> \frac{t}{5} | H_0).
% \end{align*}
% For simplicity, let $c=\frac{t}{5}$, then
% \begin{itemize}
% 	\item If $|Y|\leq c$, accept $H_0$.
% 	\item If $|Y|> c$, accept $H_1$.
% \end{itemize}
% To decode, what $c$ should be,
% \begin{align*}
% 	\alpha &= P(|Y|>c)\\ 
% 		   &= 1-P(c\leq |Y|\leq c)\\
% 		   &\approx 2(1-\Phi(c)) \\
% 	2(1-\Phi(c)) &= 0.05
% \end{align*}
% Thus, we obtain
% \begin{align*}
% 	c = \Phi^{-1}(0.975) = 1.96
% \end{align*}
% \begin{itemize}
% 	\item If $|Y|\leq 1.96$, accept $H_0$.
% 	\item If $|Y|> 1.96$, accept $H_1$.
% \end{itemize}
% The set $A = [-1.96, 1.96]$ is called the \textit{acceptance region}, since it includes the points that result in accepting $H_0$. Similarly, the set $R = (-\infty, -1.96)\cup (1.96, \infty)$ is called the \textit{rejection region}. 
% \begin{figure}[t]
% 	\centering
% 	\includegraphics[scale=1.0]{./images/acceptance.png}
% \end{figure}
% Since, $Y = \frac{X-50}{5}$, we can say that 
% \begin{itemize}
% 	\item If $|X-50|\leq 9.8$, accept $H_0$.
% 	\item If $|X-50|> 9.8$, accept $H_1$.
% \end{itemize}
% Equivalently, 
% \begin{itemize}
% 	\item $\{41, \dots, 59\}$, accept $H_0$.
% 	\item Otherwise, reject $H_0$
% \end{itemize}

% Note that the second possible error is that we accept $H_0$ when $H_0$ is false. This is called \textit{type II error}. Since the alternative hypothesis, $H_1$ is typically a composite hypothesis (\ie it can be more than one value. In contrast, simple hypothesis denotes only one element.). Thus, the probability of type II error is commonly a function of $\theta$, which is given by
% \begin{align*}
% 	\beta(\theta) = P(\text{accept }H_0)
% \end{align*}

% \begin{figure}[t]
% 	\centering
% 	\includegraphics[scale=1.0]{./images/acceptance_errors.png}
% 	\caption{Type I and II errors.}
% \end{figure}

% \section{P-Values}
% The hypothesis test provides only a decision of ``accept'' or ``reject''. However, we could indicate how close the decision was. More specifically, suppose we end up rejecting $H_0$ at at significance level $\alpha=0.05$. Then we can ask: ``How about if we require significance level $\alpha=0.01$?'' Can we still reject $H_0$? More specifically, we can ask the following question:
% $$\textit{What is the lowest significance level $\alpha$ that results in rejecting the null hypothesis?}$$
% To answer the question, we are going to use $p$-value. The $p$-value is the \textit{lowest significance level $\alpha$ that results in rejecting the null hypothesis} (\ie probability of type I error). 

% \textbf{Intuitively, if the $p$-value is small, it means that the observed data (type I error) is very unlikely to have occurred under $H_0$, thus we can be more confident in rejecting the null hypothesis}.  


% \section{Likelihood Ratio Tests}
% Let $X_1, \dots , X_n$ be a random sample from a distribution with a parameter $\theta$. Suppose that we have observed $X_1=x_1, \dots, X_n=x_n$. 
% \begin{itemize}
% 	\item If the $X_i$'s are discrete, then the likelihood function is defined as
% 		$$L(x_1,\dots,x_n;\theta) = P_{X_1,\dots,X_n}(x_1,\dots,x_n;\theta).$$
% 	\item If the $X_i$'s are continuous, then the likelihood function is defined as
% 		$$L(x_1,\dots,x_n;\theta) = f_{X_1,\dots,X_n}(x_1,\dots,x_n;\theta).$$
% \end{itemize}
% Consider a hypothesis testing problem in which both the null and the alternative hypotheses are simple:
% \begin{itemize}
% 	\item $H_0: \theta=\theta_0$
% 	\item $H_1: \theta=\theta_1$
% \end{itemize}
% One way to decide a hypothesis is to compare their corresponding likelihood functions by
% \begin{align*}
% 	\lambda(x_1,\dots,x_n) = \frac{L(x_1,\dots,x_n;\theta_0)}{L(x_1,\dots,x_n;\theta_1)}.
% \end{align*}
% If $\lambda<c$ then we reject $H_0$ and accept it if $\lambda>c$.

