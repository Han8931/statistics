\chapter{Limit Theorems and Convergence of Random Variables}

\section{Law of Large Numbers}
\textit{The law of large numbers} has a very central role in probability and statistics. It states that \textbf{if you repeat an experiment independently a large number of times and average the result, what you obtain should be close to the expected value.} There are two main versions of the law of large numbers. They are called the \textit{weak and strong laws of the large numbers}. 

For i.i.d. random variables $X_1,\dots, X_n$, the sample mean, denoted by $\overline{X}$, is defined as
\begin{align*}
	\overline{X} = \frac{X_1+\dots +X_n}{n}.
\end{align*}
Another common notation for the sample mean is $M_n$. If the $X_i$'s have CDF $F_X(x)$, we might show the sample mean by $M_n(X)$ to indicate distribution of the $X_i$s.

Note that since the $X_i$s are random variables, the sample mean, $\overline{X} = M_n(X)$, is also a random variable. In particular we have
\begin{align*}
	E\overline{X} &= \frac{EX_1+\dots+EX_n}{n} \quad \text{by linearity of expectation}\\
				  &= \frac{nEX}{n} \quad \text{Since they are i.i.d., $EX_i=EX$}\\
				  &= EX.
\end{align*}
Also the variance of $\overline{X}$ is given by
\begin{align*}
	\text{Var}(\overline{X}) &= \frac{\text{Var}(X_1+\dots+X_n)}{n^2} \quad \text{Since Var$(aX)$=$a^2$Var$(X)$}\\
							 &= \frac{\text{Var}(X_1)+\dots+\text{Var}(X_n)}{n^2} \quad \text{Since $X_i$s are independent}\\
							 &= \frac{n\text{Var}(X)}{n^2}\\
							 &= \frac{\text{Var}(X)}{n}.
\end{align*}
The weak law of large numbers (WLLN) states that for any $\epsilon >0$, i.i.d. random variables $X_1,\dots, X_n$ with a finite expected value $EX_i=\mu<\infty$, 
\begin{align*}
	\lim_{n\to\infty}P(|\overline{X}-\mu|\geq \epsilon) = 0.
\end{align*}

\section{Central Limit Theorems}
\textit{The central limit theorem} (CLT) is one of the most important results in probability theory. It states that, \textbf{under certain conditions, the sum of a large number of random variables is approximately normal}.

Suppose that $X_1,\dots,X_n$ are i.i.d. random variables with expected values $EX_i=\mu<\infty$ and variance Var$(X_i)=\sigma^2<\infty$. Then as we saw above, the sample mean and variance have $E\overline{X}=\mu$ and Var$(\overline{X})=\frac{\sigma^2}{n}$. Thus, the normalized random variable

\begin{align*}
	Z_n = \frac{\overline{X}-\mu}{\frac{\sigma}{\sqrt{n}}} = \frac{X_1+\dots+X_n-n\mu}{\sqrt{n}\sigma}
\end{align*}
has zero mean $EZ_n=0$ and variance Var$(Z_n)=1$. The central limit theorem states that the CDF of $Z_n$ converges to the standard normal CDF as $n$ goes to infinity, that is
\begin{align*}
	\lim_{n\to \infty} P(Z\leq x)\Phi(x), \quad \text{for all }x\in \mathbb{R},
\end{align*}
where $\Phi(x)$ is the standard normal CDF.

In sum CLT states that \textbf{the CDF of $Z_n$ is converging to the CDF of $\mathcal{N}(0,1)$.}

The importance of the central limit theorem stems from the fact that, in many real applications, a certain random variable of interest is a sum of a large number of independent random variables. In these situations, we are often able to use the CLT to justify using the normal distribution. Examples of such random variables are found in almost every discipline. Here are a few:
\begin{itemize}
	\item Laboratory measurement errors are usually modeled by normal random variables.
	\item In communication and signal processing, Gaussian noise is the most frequently used model for noise.
	\item In finance, the percentage changes in the prices of some assets are sometimes modeled by normal random variables.
	\item When we do random sampling from a population to obtain statistical knowledge about the population, we often model the resulting quantity as a normal random variable.
\end{itemize}

\section{Convergence of Random Variables}

We would like to see if a sequence of a random variables $X_1, X_2 \dots $ converges to a random variable $X$. That is, we would like to see if $X_n$ gets closer and closer to $X$ as $n$ increases.

In fact, we have already seen the concept of convergence when we discussed limit theorems (the weak law of large numbers (WLLN) and the central limit theorem (CLT)). The WLLN states that the average of a large number of i.i.d. random variables converges in probability to the expected value. The CLT states that the normalized average of a sequence of i.i.d. random variables converges in distribution to a standard normal distribution. 

\subsection{Convergence of a Sequence of Numbers}

Let's say we have a sequence of a real numbers $a_1, a_2,\dots $, which is defined as $a_n=\frac{n}{n+1}$. Then, we can ask whether the sequence converges. This sequence converges to 1. We say that a sequence converges to a limit $L$ if $a_n$ approaches to $L$ as $n$ goes to infinity. That is, for any $\epsilon>0$, there exists an $N\in \mathbb{N}$ such that
\begin{align*}
	\left|a_n-L\right|<\epsilon, \quad \text{for all }n>N.
\end{align*}

\subsection{Sequence of Random Variables}
In any probability model, we have a sample space $S = \{s_1,\dots,s_k\}$ and a probability measure $P$. Then, a random variable $X$ is a mapping that assigns a real number to any of the possible outcomes $s_i$, $i=1,2,\dots,k$. Thus, we may write 
\begin{align*}
	X(s_i) = x_i, \quad \text{for }i=1,2,\dots, k.
\end{align*}
% When we have a sequence of random variables, $X_1, X_2, \dots, $, it is also useful to remember that we have an underlying sample space $S$. 

\subsection{Different Types of Convergence for Sequences of Random Variables}

Consider a sequence of random variables $X_1, X_2, \dots, $. This sequence might converge to a random variable $X$. There are four types of convergence that we are going to discuss:
\begin{itemize}
	\item Convergence in distribution
	\item Convergence in probability
	\item Convergence in mean
	\item Almost sure convergence
\end{itemize}
Some of these convergence types are ``stronger'' than others and some are ``weaker.'' By this, we mean the following: If Type A convergence is stronger than Type B convergence, it means that Type A convergence implies Type B convergence. 

\paragraph{Convergence in Distribution} Convergence in distribution is the weakest type of convergence in some sense. All it says is that the CDF of $X_n$'s converges to the CDF of $X$ as $n$ goes to infinity. To say that $X_n$ converges in distribution to $X$, we write
\begin{align*}
	X_n \, \xrightarrow{d}\, X .
\end{align*}

\paragraph{Convergence in Probability} Convergence in probability is stronger than convergence in distribution. In particular, for a sequence $X_1,X_2,\dots$ to converge to a random variable $X$, we must have that $P(|X_n-X|\geq \epsilon)$ goes to 0 as $n\to \infty$, for any $\epsilon>0$.
\begin{align*}
	X_n \, \xrightarrow{p}\, X .
\end{align*}

\paragraph{Convergence in Mean} One way of interpreting the convergence of a sequence $X_n$ to $X$ is to say that the ``distance'' between $X$ and $X_n$ is getting smaller and smaller. For instance, if we define the distance between $X_n$ and $X$ as $P(|X_n-X|\geq \epsilon)$, we have convergence in probability. One way to define the distance between $X_n$ and $X$ is 
\begin{align*}
	E(|X_n-X|^r),
\end{align*}
where $r\geq 1$ is a fixed number. This refers to convergence in mean. The most common choice for $r$ is 2, in which case it is called the \textit{mean square convergence}.  

\paragraph{Almost Sure Convergence}
