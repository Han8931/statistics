\chapter{Gaussian Distribution}

\section{Gaussian Distribution}
The normal distribution is by far \textbf{the most important probability distribution}. One of the main reasons for that is the \textit{Central Limit Theorem} (CLT). To give you an idea, the CLT states that \textit{if you add a large number of random variables, the distribution of the sum will be approximately normal under certain conditions}. The importance of this result comes from the fact that many random variables in real life can be expressed as the sum of a large number of random variables and, by the CLT, we can argue that distribution of the sum should be normal. The CLT is one of the most important results in probability and we will discuss it later on. Here, we will introduce normal random variables.

We first define the standard normal random variable. We will then see that we can obtain other normal random variables by scaling and shifting a standard normal random variable. 

A continuous random variable $Z$ is said to be a \textit{standard normal} (\textit{standard Gaussian}) random variable, shown as $Z\sim \mathcal{N}(0,1)$, if its PDF is given by
$$f_Z(z) = \frac{1}{\sqrt{2 \pi}} \exp\left\{-\frac{z^2}{2}\right\}, \quad \textrm{for all } z \in \mathbb{R}.$$
The $1/\sqrt{2\pi}$ is there to make sure that the area under the PDF is equal to one. 

More generally, 
$$f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}, \quad \textrm{for all } z \in \mathbb{R}.$$

\subsection{Cumulative Distribution Function}
The CDF of the standard normal distribution is denoted by the $\Phi$ function:
$$\Phi(x)=P(Z \leq x)= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{x}\exp\left\{-\frac{u^2}{2}\right\} du.$$
Here are some properties of the $\Phi$ function:
\begin{itemize}
	\item $\lim_{x\to\infty}\Phi(x) = 1$
	\item $\lim_{x\to-\infty}\Phi(x) = 0$
	\item $\Phi(0) = \frac{1}{2}$
	\item $\Phi(-x) = 1-\Phi(x), \forall x \in \mathbb{R}.$
\end{itemize}

\subsection{Multinomial}
For a $D$-dimensional vector $\rvx$, the multivariate Gaussian distribution takes the form
\begin{align}
	\mathcal{N}(\mathbf{x}|\boldsymbol{\mu},\boldsymbol{\Sigma}) &= \frac{1}{(2\pi)^{D/2}|\boldsymbol{\Sigma}|^{1/2}}\exp\bigg(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\bigg)
	\label{eq:normal_distribution}
\end{align}
The multivariate Gaussian distribution is fully characterized by a mean vector and a covariance matrix.

\subsection{Marginal and Conditional Gaussian Distributions}

Consider first the case of conditional distributions. Suppose $\rvx$ is a $D$-dimensional vector with Gaussian distribution $\mathcal{N}(\mathbf{x}|\boldsymbol{\mu},\boldsymbol{\Sigma})$ and that we partition $\rvx$ into two disjoint subsets $\rvx_a$ and $\rvx_b$. Thus, $\rvx_a$ has $M$ components and $\rvx_b$ has $D-M$ components.
\begin{align*}
	\rvx = \begin{bmatrix}
		\rvx_a\\
		\rvx_b
	\end{bmatrix}.
\end{align*}
Similarly, 
\begin{align*}
	\boldsymbol{\mu} = \begin{bmatrix}
		\boldsymbol{\mu}_a\\
		\boldsymbol{\mu}_b
	\end{bmatrix} 
\end{align*}
and the covariance matrix is given by
\begin{align*}
	\boldsymbol{\Sigma} = \begin{bmatrix}
		\boldsymbol{\Sigma}_{aa} & \boldsymbol{\Sigma}_{ab}\\
		\boldsymbol{\Sigma}_{ba} & \boldsymbol{\Sigma}_{bb} 
	\end{bmatrix} 
\end{align*}
Note that the symmetry $\mSigma^T = \mSigma$ implies that $\mSigma_{ab}^T=\mSigma_{ba}$. 
\begin{align*}
	p(\rvx, \rvy) = \mathcal{N}\left(
	\begin{bmatrix}
		\boldsymbol{\mu}_x\\
		\boldsymbol{\mu}_y
	\end{bmatrix},
	\begin{bmatrix}
		\boldsymbol{\Sigma}_{xx} & \boldsymbol{\Sigma}_{xy}\\
		\boldsymbol{\Sigma}_{yx} & \boldsymbol{\Sigma}_{yy} 
	\end{bmatrix} 
	\right)
\end{align*}
The conditional distribution $p(\rvx|\rvy)$ is also Gaussian and given by
\begin{align*}
	p(\rvx|\rvy) &= \mathcal{N}(\boldsymbol{\mu}_{x|y},\boldsymbol{\Sigma}_{x|y})\\
	\boldsymbol{\mu}_{x|y}&= \boldsymbol{\mu}_{x}+\boldsymbol{\Sigma}_{xy}\boldsymbol{\Sigma}_{yy}^{-1}(\rvy-\boldsymbol{\mu}_{y})\\
	\boldsymbol{\Sigma}_{x|y}&= \boldsymbol{\Sigma}_{xx}-\boldsymbol{\Sigma}_{xy}\boldsymbol{\Sigma}_{yy}^{-1}\boldsymbol{\Sigma}_{yx}.
\end{align*}
Note that the mean of the conditional distribution $p(\rvx|\rvy)$ is a linear function of $\rvy$ and that the covariance, is independent of $\rvx$. 

Note that the both the conditional and marginal covariance are equal only when $\boldsymbol{\Sigma}_{xy}=0$. This happens when $\rvx$ and $\rvy$ are uncorrelated.

The mean of the conditional distribution depends explicitly on new measurements ($\rvy$) whereas the conditional covariance does not depend on the new measurements. This means that we can know how our error will change once we receive our measurements, even before the measurements arrive.

\subsection{Sampling from Multivariate Gaussian Distributions}

To obtain samples from a multivariate normal $\mathcal{N}(\mathbf{x}|\boldsymbol{\mu},\boldsymbol{\Sigma})$, we can use the properties of a linear transformation of a Gaussian random variable: If $\rvx \sim \mathcal{N}(0,I)$, then $\rvy = A\rvx + \boldsymbol{\mu}$, where $AA^T = \boldsymbol{\Sigma}$ is Gaussian distributed with mean and covariance matrix. One convenient choice of $A$ is to use the Cholesky decomposition of the covariance matrix. The Cholesky decomposition has the benefit that $A$ is triangular, leading to efficient computation: 
\begin{align*}
	Cov(\rvy) = Cov(A\rvx) = ACov(\rvx)A^T = AIA^T = AA^T = \boldsymbol{\Sigma}.
\end{align*}
Thus, we can use the Cholesky decomposition.



% We can also define a \textit{precision matrix} as follows: 
% $$\mLambda \equiv \mSigma^{-1},$$
% Note that the precision is the inverse of variance (\ie 1/Variance). A higher precision indicates more certainty or confidence in the estimate, meaning the data points are less spread out.

% We also introduce a partitioned form of the precision matrix:
% \begin{align}
% 	\boldsymbol{\Lambda} = \begin{bmatrix}
% 		\mLambda_{aa} & \mLambda_{ab}\\
% 		\mLambda_{ba} & \mLambda_{bb} 
% 	\end{bmatrix} 
% 	\label{eq:precision_matrix}
% \end{align}
% Because the inverse of a symmetric matrix is also symmetric, we see that $\mLambda_{aa}$ and $\mLambda_{bb}$ are symmetric and $\mLambda_{ab}^T =\mLambda_{ba}$. Note that, for instance, $\mLambda_{aa}$ is not simply given by the inverse of $\boldsymbol{\Sigma}_{aa}$. 

% Now let's compute the conditional probability:
% \begin{align*}
% 	-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}) &= \frac{1}{2}\Bigg(\bigg(\begin{bmatrix}
% 		\rvx_a\\
% 		\rvx_b
% 	\end{bmatrix}-
% 	\begin{bmatrix}
% 		\boldsymbol{\mu}_a\\
% 		\boldsymbol{\mu}_b
% 	\end{bmatrix} 
% \bigg)^T\begin{bmatrix}
% 		\mLambda_{aa} & \mLambda_{ab}\\
% 		\mLambda_{ba} & \mLambda_{bb} 
% 	\end{bmatrix}\bigg(\begin{bmatrix}
% 		\rvx_a\\
% 		\rvx_b
% 	\end{bmatrix}-
% 	\begin{bmatrix}
% 		\boldsymbol{\mu}_a\\
% 		\boldsymbol{\mu}_b
% 	\end{bmatrix}\bigg)\Bigg)\\
% 	&= -\frac{1}{2}\big((\rvx_a-\vmu_a)^T\mLambda_{aa}(\rvx_a-\vmu_a)+(\rvx_a-\vmu_a)^T\mLambda_{ab}(\rvx_a-\vmu_a)\\
% 	&\quad+(\rvx_a-\vmu_a)^T\mLambda_{ba}(\rvx_a-\vmu_a)+(\rvx_a-\vmu_a)^T\mLambda_{bb}(\rvx_a-\vmu_a)\big)\\
% \end{align*}



