\chapter{Joint Distributions}

In real life, we are often \textbf{interested in several random variables that are related to each other.} For example, suppose that we choose a random family, and we would like to study the number of people in the family, the household income, the ages of the family members, etc. Each of these is a random variable, and we suspect that they are dependent. In this chapter, we develop tools to study joint distributions of random variables. The concepts are similar to what we have seen so far. The only difference is that instead of one random variable, we consider two or more. In this chapter, we will focus on two random variables, but once you understand the theory for two random variables, the extension to $n$ random variables is straightforward. We will first discuss joint distributions of discrete random variables and then extend the results to continuous random variables.

\section{Joint PMF}
Recall that for a discrete RV $X$, we define the PMF as $P_X(x) = P(X=x)$. Now, if we have two RVs $X$ and $Y$, we define the joint PMF as follows:
$$P_{XY}(x,y) = P(X=x, Y=y).$$
Note that the comma means ``and'', so we can write as 
\begin{align*}
	P_{XY}(x,y) &= P(X=x, Y=y)\\
				&= P(X=x \text{ and } Y=y)\\
				&= P(X=x \cap Y=y)
\end{align*}
We can define the joint range for $X$ and $Y$ as
$$R_{XY} = \{(x,y)|P_{XY}(x,y)>0\}.$$
In particular, if $R_{X} = \{x_1,x_2,\dots\}$ and $R_{Y} = \{y_1,y_2,\dots\}$, then 
\begin{align*}
	R_{XY} &\subset R_X\times R_Y\\ 
		   &= R_{XY} = \{(x_i,y_j)|x_i\in R_{X}, y_j\in R_{Y}\}.
\end{align*}
For two discrete RVs, we have
$$\sum_{(x_i,y_j)\in R_{XY}}P_{XY}(x_i,y_j) = 1$$
We can use the joint PMF to find $P((X,Y)\in A)$ for any set $A\subset \mathbb{R}^2$. Specifically, we have
$$P((X,Y)\in A) = \sum_{(x_i,y_j)\in (A\cap R_{XY})}P_{XY}(x_i,y_j)$$

\section{Joint CDF}
The joint cumulative distribution function of two random variables $X$ and $Y$ is defined as
$$F_{XY}(x,y) = P(X\leq x, Y\leq y).$$
Equivalently, 
\begin{align*}
	F_{XY}(x,y) &= P(X\leq x, Y\leq y)\\
				&= P(X\leq x \cap Y\leq y)
\end{align*}
If we know the CDF of $X$ and $Y$, we can find the \textit{marginal} CDFs, $F_X(x)$ and $F_Y(y)$. Specifically, for any $x\in \mathbb{R}$, we have
\begin{align*}
	F_{XY}(x,\infty) &= P(X\leq x, Y\leq \infty)\\
				&= P(X\leq x)\\
				&= F_X(x)
\end{align*}

\section{Conditioning and Independence}
\begin{align*}
	 P(A|B)=\frac{P(A \cap B)}{P(B)},\, \textrm{ when } P(B)>0.
\end{align*}

\subsection{Conditional PMF and CDF}
The conditional PMF of $X$ given an event $A$ is given by
\begin{align*}
	P_{X|A}(x_i) &= P(X=x_i|A)\\
				 &= \frac{P(X=x_i \text{ and }A)}{P(A)}
\end{align*}
Similarly, 
\begin{align*}
	F_{X|A}(x) = P(X\leq x|A)
\end{align*}

\subsection{Conditional PMF of $X$ given $Y$}
We have observed the value of a random variable $Y$, and we need to update the PMF of another random variable $X$ whose value has not yet been  observed. In these problems, we use the conditional PMF of $X$ given $Y$:  
\begin{align*}
	P_{X|Y}(x_i|y_j) &= P(X=x_i|Y=y_j)\\
				 &= \frac{P(X=x_i, Y=y_j)}{P(Y=y_j)}\\
				 &= \frac{P_{XY}(x_i,y_j)}{P_Y(y_j)}
\end{align*}

\subsection{Independent Random Variables}
Two discrete RVs $X$ and $Y$ are independent if 
$$P_{XY}(x,y) = P_X(x)P_Y(y), \forall x, y.$$
Equivalently, 
$$F_{XY}(x,y) = F_X(x)F_Y(y), \forall x, y.$$

If $X$ and $Y$ are independent, 
$$P_{X|Y}(x_i|y_j) = P_X(x_i).$$

\subsection{Conditional Expectation}
Given that we know an event $A$ has occurred, we can compute the conditional expectation of a RV $X$, $E[X|A]$: 
\begin{align*}
	E[X|A] = \sum_{x_i\in R_X}x_iP_{X|A}(x_i).
\end{align*}
Similarly, given that we have observed the value of random variable $Y$, we can compute the conditional expectation of $X$:
\begin{align*}
	E[X|Y=y] = \sum_{x_i\in R_X}x_iP_{X|Y}(x_i|y).
\end{align*}

\section{The Law of Total Probability}
Recall that the law of total probability: If $B_1,B_2,\dots$ is a partition of the sample space $S$, then for any event $A$ we have
\begin{align*}
	P(A) = \sum_i P(A\cap B_i) = \sum_i P(A| B_i)P(B_i).
\end{align*}
If $Y$ is a discrete random variable with range $R_y=\{y_1,y_2,\dots\}$, then the events $\{Y=y_1\}, \{Y=y_2\}, \dots$,  form a partition of the sample space. Thus, we can use the law of total probability:
\begin{align*}
	P_X(x) = \sum_{y_j\in R_Y}P_{XY}( x,y_j ) = \sum_{y_j\in R_Y}P_{X|Y}( x|y_j )P_Y(y_j).
\end{align*}
We can write this more generally as
\begin{align*}
	P(X\in A) = \sum_{y_j\in R_Y}P( X\in A | Y=y_j ) P_Y(y_j), \, \text{ for any set} A.
\end{align*}
Similarly, we can write the law of total expectation:
\begin{align*}
	EX &= \sum_{i}E[X|B_i]P(B_i)\\
	EX &= \sum_{y_j\in R_Y}E[X|Y=y_i]P_Y(y_j).
\end{align*}
This means that the expected value of $X$ can be calculated from the probability distribution of $X|Y$ and $Y$, which is often useful both in theory and practice.

\section{Functions of Two Random Variables}
Suppose that you have two discrete random variables $X$ and $Y$, and suppose that $Z = g(X,Y)$, where $g: \mathbb{R}^2\to\mathbb{R}$. Then, the PMF of $Z$ is given by
\begin{align*}
	P_Z(z) &= P(g(X,Y)=z)\\
		   &= \sum_{(x_i,y_j)\in A_z}P_{XY}(x_i,y_j), 
\end{align*}
where $A_z = \{(x_i,y_j)\in R_{XY}: g(x_i,y_j) = z\}$. Note that if we are only interested in $E[g(X,Y)]$, we can directly use LOTUS, without finding $P_Z(z)$:
\begin{align*}
	E[g(X,Y)] = \sum_{(x_i,y_j)\in R_{XY}} g(x_i,y_j)P_{XY}(x_i,y_j).
\end{align*}

\section{Conditional Expectation and Conditional Variance}

\subsection{Conditional Expectation as a Function of a Random Variable}

Note that 
\begin{itemize}
	\item $E[X]$ is a scalar value
	\item $E[X|Y]$ is a random variable, because the value depends on $Y$.
\end{itemize}
\begin{align*}
	E[X] &= \sum_x x \cdot p(x)\\
	E[E[X|Y]] &= E[X]
\end{align*}
Since, $E[E[X|Y]]$ is the function of $Y$. It is also called \textit{the law of iterated expectations}.
\begin{align*}
	\mathbb{E}[\mathbb{E}[X|Y]]&= \mathbb{E}\Bigg[\sum_{x}x\cdot P(X=x|Y)\Bigg]\\
	&= \sum_y\Bigg[\sum_{x}x\cdot P(X=x|Y)\Bigg]\cdot P(Y=y)\\
	&= \sum_y\sum_{x}x\cdot P(X=x,Y)\\
	&= \sum_x x\sum_{y}\cdot P(X=x,Y)\\
	&= \sum_x x\cdot P(X=x)\\
	&= \mathbb{E}[X]
\end{align*}
\begin{align*}
E[Y \mid X = x] &= \sum_y y\cdot p_{Y\mid X}(y\mid X = x)\\
&= \sum_y y \cdot \frac{p_{X,Y}(x,y)}{p_X(x)}\\
&= \sum_y y \cdot \frac{\sum_z p_{X,Y,Z}(x,y,z)}{p_X(x)}\\
&= \sum_y y \cdot \frac{\sum_z p_{Y\mid X,Z}(y \mid X=x, Z=z)\cdot p_{X,Z}(x,z)}{p_X(x)}\\
&= \sum_z \frac{p_{X,Z}(x,z)}{p_X(x)}\sum_y y \cdot p_{Y\mid X,Z}(y \mid X=x, Z=z)\\
&= \sum_z p_{Z\mid X}(z \mid X=x)\cdot \sum_y y \cdot p_{Y\mid X,Z}(y \mid X=x, Z=z)\\
&= \sum_z p_{Z\mid X}(z \mid X=x)\cdot E[Y \mid X=x, Z=z)\\
&= E\left[E[Y\mid X,Z]\mid X = x\right]
\end{align*}

Note that if $X$ and $Y$ are independent,
\begin{itemize}
	\item $E[X|Y] = EX$. 
	\item $E[g(X)|Y] = E[g(X)]$. 
	\item $E[XY] = EXEY$. 
	\item $E[g(X)h(Y)] = E[g(X)]E[h(Y)]$. 
\end{itemize}

\subsection{Conditional Variance}
We can define the conditional variance of $X$, $Var(X|Y=y)$. Let $\mu_{X|Y}(y) = E[X|Y=y]$, then
\begin{align*}
	Var(X|Y=y) &= E[\left(X-\mu_{X|Y}(y)\right)^2|Y=y]\\
			   &= \sum_{x_i\in R_X}\left(x_i-\mu_{X|Y}(y)\right)^2P_{X|Y}(x_i)\\
			   &= E[X^2|Y=y] - \mu_{X|Y}(y)^2
\end{align*}
Note that $Var(X|Y=y)$ is a function of $y$.  

\section{Two Continuous Random Variables}

\subsection{Joint Probability Density Function}

Two random variables are jointly continuous if they have a joint probability density function as follows:

Two random variables $X$ and $Y$ are jointly continuous if there exists a non-negative function $f_{XY}: \mathbb{R}^2\to \mathbb{R}$, such that, for any set $A\in \mathbb{R}^2$, we have
\begin{align*}
	P((X,Y)\in A) = \iint\limits_{A}f_{XY}(x,y)dxdy.
\end{align*}
The function $f_{XY}(x,y)$ is called the joint probability density function of $X$ and $Y$. The domain of $f_{XY}(x,y)$ is the entire $\mathbb{R}^2$ and the range is
$$R_{XY} = \{(x,y)|f_{XY}(x,y)>0\}.$$
The intuition behind the joint density is similar to that of the PDF of a single random variable. Recall that a random variable $X$ and small positive $\delta$, we have
$$P(x<X\leq x+\delta)\approx f_X(x)\delta.$$
Similarly, for small $\delta_x$ and $\delta_y$, 
$$P(x<X\leq x+\delta_x, y\leq Y\leq y+\delta_y)\approx f_{XY}(x,y)\delta_x\delta_y.$$

\subsection{Joint CDF}

\subsection{Conditioning and Independence}

\subsection{Functions of Two Continuous Random Variables}

LOTUS for two continuous random variables:
\begin{align*}
	E[g(X,Y)]=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y)f_{XY}(x,y) dxdy 
\end{align*}

\paragraph{The Method of Transformations}

\begin{theorem}{The Method of Transformations}
	Let $X$ and $Y$ be two jointly continuous random variables. Let $(Z,W) = g(X,Y) = (g_1(X,Y), g_2(X,Y))$, where $g: \mathbb{R}^2\to \mathbb{R}^2$ is a continuous one-to-one (invertible) function with continuous partial derivatives. Let $h=g^{-1}$, \ie $(X,Y) = h(Z,W) = (h_1(Z,W), h_2(Z,W))$. Then $Z$ and $W$ are jointly continuous and their joint PDF, $f_{ZW}(z,w)$ for $(z,w)\in R_{ZW}$ is given by 
	$$f_{ZW}(z,w) = f_{XY}(h_1(z,w), h_2(z,w))|J|,$$
	where $J$ is the Jacobian of $h$ defined by 
	\begin{align*}
		J &= \text{det}\begin{bmatrix}
			\frac{\partial h_1}{\partial z} & \frac{\partial h_1}{\partial w}\\
			\frac{\partial h_2}{\partial z} & \frac{\partial h_2}{\partial w}
		\end{bmatrix}\\
		  &= \frac{\partial h_1}{\partial z}\cdot \frac{\partial h_2}{\partial w} - \frac{\partial h_2}{\partial z}\frac{\partial h_w}{\partial w}.
	\end{align*}
\end{theorem}

\paragraph{Example} Let $X$ and $Y$ be two independent standard normal RVs. Let 
\begin{align*}
	\begin{cases}
		Z&= 2X-Y\\
		W&= -X+Y
	\end{cases}
\end{align*}
Find $f_{ZW}(z, w)$.

$X$ and $Y$ are jointly continuous and their joint PDF is given by
$$f_{XY}(x,y) = f_X(x)f_Y(y)$$
Here, the function $g$ is defined by $(z,w) = g(x,y) = (g_1(x,y), g_2(x,y)) = (2x-y,-x+y)$. We can obtain the inverse function $h$:
\begin{align*}
	\begin{cases}
		x &= z+w = h_1(z,w)\\
		y &= z+2w =  h_w(z,w)
	\end{cases}
\end{align*}

\paragraph{Example} Let $X$ and $Y$ be two RVs with joint PDF  $f_{XY}(x,y)$. Let $Z = X+Y$. Find $f_Z(z)$.

To apply the above theorem, we need two random variables $Z$ and $W$.  We can simply define $W=X$. Then, we get
\begin{align*}
	\begin{cases}
		z &= x+y \\
		w &= x
	\end{cases}
\end{align*}
Then, we can find the inverse transform:
\begin{align*}
	\begin{cases}
		x &= w \\
		y &= z-w
	\end{cases}
\end{align*}
Thus, 
$$f_{ZW}(z,w) = f_{XY}(w,z-w).$$
However, we are interested in the marginal PDF, $f_Z(z)$, we can get it by
$$f_Z(z) = \int_{-\infty}^{\infty}f_{XY}(w, z-w)dw.$$
Note that if $X$ and $Y$ are independent, then $f_{XY}(x,y) = f_{X}(x)f_{Y}(y)$ and we conclude that 
$$f_Z(z) = \int_{-\infty}^{\infty}f_{X}(w)f_{Y}(z-w)dw.$$
The above integral is called the \textit{convolution} of $f_X$ and $f_Y$, and we write
\begin{align*}
	f_Z(z) &= f_X*f_Y\\
		   &= \int_{-\infty}^{\infty}f_{X}(w)f_{Y}(z-w)dw = \int_{-\infty}^{\infty}f_{Y}(w)f_{X}(z-w)dw
\end{align*}
\textbf{The convolution can be thought as a sum of two independent RVs.}

\paragraph{Example} Let $X,Y\sim \text{Unif}[1,4]$ be independent rolls of a fair 4-sided die. What is the PMF of $Z=X+Y$.
We know that for the range of $Z$ we have the following, since it is the sum of two values each in the range $\{2, 3, 4, 5, 6, 7, 8\}$.

f I wanted to compute the probability that $Z=3$ for example, I could just sum over all possible values of $X$ in $\Omega_X = \{1, 2, 3, 4\}$ to get:
\begin{align*}
	P(Z=3) &= P(X=1, Y=2)+P(X=2,Y=1)+ \dots + P(X=4, Y=-1) \\
		   &= P(X=1)P(Y=2)+P(X=2)P(Y=1)+ \dots + P(X=4)P(Y=-1) \\
		   &= \frac{1}{4}\cdot \frac{1}{4}+ \dots + \frac{1}{4}\cdot 0\\
		   &= \frac{2}{16}
\end{align*}
Thus, 
\begin{align*}
	P(Z=z) &= \sum_{x\in \Omega_X}P(X=x, Y=z-x)\\
		   &= \sum_{x\in \Omega_X}P(X=x)P(Y=z-x)
\end{align*}

\paragraph{Example} Let $X$ and $Y$ be two independent standard normal RVs, and let $Z = X+Y$. Find the PDF of $Z$.

We have 
\begin{align*}
	f_Z(z) &= f_X(x)+ f_Y(y)\\
		   &= \int_{-\infty}^{\infty}f_X(w)f_Y(z-w)dw\\
		   &= \int_{-\infty}^{\infty}\frac{1}{2\pi}e^{-\frac{w^2}{2}}e^{-\frac{(z-w)^2}{2}}dw\\
		   &= \frac{1}{\sqrt{4\pi}}e^{-\frac{z^2}{4}}\int_{-\infty}^{\infty}\frac{1}{\sqrt{\pi}}e^{-\left(w-\frac{z}{2}\right)^2}dw\\
		   &= \frac{1}{\sqrt{4\pi}}e^{-\frac{z^2}{4}}
\end{align*}


Note that if $X\sim \mathcal{N}\left(\mu_X, \sigma_X^2\right)$ and $Y\sim \mathcal{N}\left(\mu_Y, \sigma_Y^2\right)$ are independent, then 
$$X+Y\sim \mathcal{N}\left(\mu_X+\mu_Y, \sigma_X^2+\sigma_Y^2\right).$$

\section{Covariance and Correlation}

Consider two RVs $X$ and $Y$. Here, we define the covariance between $X$ and $Y$, $\text{Cov}(X,Y)$. The covariance gives some information about how $X$ and $Y$ are statistically related. The covariance between $X$ and $Y$ is defined as
\begin{align*}
	\text{Cov}(X,Y) = E[(X-EX)(Y-EY)] = E[XY]-(EX)(EY)
\end{align*}
Note that
\begin{align*}
	E[(X-EX)(Y-EY)] &= E[XY-X(EY)-(EX)Y+(EX)(EY)]\\
					&= E[XY]-(EX)(EY)-(EX)(EY)+(EX)(EY)]\\
					&= E[XY]-(EX)(EY).
\end{align*}
Intuitively, the covariance between $X$ and $Y$ indicates how the values of $X$ and $Y$ move relative to each other. If large values of $X$ tend to happen with large values of $Y$, then the covariance is positive and we say $X$ and $Y$ are positively correlated. 

The covariance has the following properties:
\begin{itemize}
	\item $\text{Cov}(X,X) = \text{Var}(X)$ 
	\item If $X$ and $Y$ are independent, then $\text{Cov}(X,Y) = 0$, since $E[XY] = EXEY$, so it is zero.
	\item $\text{Cov}(X,Y) = \text{Cov}(Y,X)$ 
	\item $\text{Cov}(aX,Y) = a\text{Cov}(X,Y)$ 
	\item $\text{Cov}(X+c,Y) = \text{Cov}(X,Y)$ 
	\item $\text{Cov}(X+Y,Z) = \text{Cov}(X,Z)+\text{Cov}(Y,Z)$ 
	\item More generally, 
		\begin{align*}
			\text{Cov}\left(\sum_{i=1}^{m}a_iX_i,\sum_{j=1}^{n}b_jY_j\right) = \sum_{i=1}^{m}\sum_{j=1}^{n}a_ib_j\text{Cov}(X_i,Y_j).
		\end{align*}
\end{itemize}

\subsection{Variance of a Sum}
\begin{align*}
	\text{Var}(aX+bY) = a^2\text{Var}(X)+b^2\text{Var}(Y)+2ab\text{Cov}(X,Y)
\end{align*}

\subsection{Correlation Coefficient}
The \textit{correlation coefficient}, denoted by $\rho_{XY}$ or $\rho(X,Y)$ is obtained by normalizing the covariance. We can define the correlation coefficient of two random variables $X$ and $Y$ as the covariance of the standardized versions of $X$ and $Y$,
\begin{align*}
	\rho_{XY} &= \text{Cov}(\frac{X-EX}{\sigma_X}, \frac{Y-EY}{\sigma_Y}) \\
			  &= \text{Cov}(\frac{X}{\sigma_X}, \frac{Y}{\sigma_Y})\quad \text{by the property of Cov}\\
			  &= \frac{\text{Cov}(X,Y)}{\sigma_X\sigma_Y}
\end{align*}
A nice property of the correlation coefficient is that it is always between -1 and 1. This is an immediate result of \textit{Cauchy-Schwarz inequality}. One way to prove that $-1\leq \rho\leq 1$ is to use the following inequality:
\begin{align*}
	\alpha\beta\leq \frac{\alpha^2+\beta^2}{2}, \quad \text{for }\alpha,\beta\in \mathbb{R}.
\end{align*}
This is because $(\alpha-\beta)^2\geq 0$. The equality holds only when $\alpha=\beta$. From this, we can conclude that for any two random variables $U$ and $V$, which are the standardized versions of $X$ and $Y$, respectively: 
\begin{align*}
	E[UV]\leq \frac{EU^2+EV^2}{2}.
\end{align*}
By the definition, $\rho_{XY} = \text{Cov}(U,V) = E[UV]$. Note that $EU^2=EV^2=1$ by definition, so we get
\begin{align*}
	\rho_{XY} = E[UV]\leq \frac{EU^2+EV^2}{2} = 1,
\end{align*}
with equality only if $U=V$. 


Note that two independent random variables are always uncorrelated, but the converse is not necessary true. In other words, if $X$ and $Y$ are uncorrelated, then $X$ and $Y$ may or may not be independent.


\section{Bivariate Normal Distribution}

\subsection{Mixed Case}
The mixed joint density may be defined where one or more random variables are continuous and the other random variables are discrete. With one variable of each type
\begin{align*}
	f_{XY}(x,y) = f_{X|Y}(x|y)P_Y(Y=y) = P(Y=y|X=x)f_X(x)
\end{align*} 







