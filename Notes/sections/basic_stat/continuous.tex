\chapter{Continuous and Mixed Random Variables}

\section{Introduction}
Remember that discrete random variables can take only a countable number of possible values. On the other hand, a continuous random variable $X$ has a range in the form of an interval or a union of non-overlapping intervals on the real line (possibly the whole real line). Also, for any $x\in \mathbb{R}$, $P(X=x)=0$. Thus, we need to develop new tools to deal with continuous random variables. The good news is that the theory of continuous random variables is completely analogous to the theory of discrete random variables. Indeed, if we want to oversimplify things, we might say the following: take any formula about discrete random variables, and then replace sums with integrals, and replace PMFs with probability density functions (PDFs), and you will get the corresponding formula for continuous random variables. 

\section{Probability Density Function (PDF)}

To determine the distribution of a discrete random variable we can either provide its PMF or CDF. For continuous random variables, the CDF is well-defined so we can provide the CDF. However, \textbf{the PMF does not work for continuous random variables, because for a continuous random variable} $P(X=x)=0,\ \forall x\in \mathbb{R}$. Instead, we can usually define the \textit{probability density function} (PDF). The PDF is the $density$ of probability rather than the probability mass. The concept is very similar to mass density in physics: its unit is probability per unit length. For example, let the bus waiting time be uniformly distributed: $X\sim[10,30]$. The probability of waiting between 15 and 20 minutes is: 
$$P(X<15)=\int_{15}^{20} \frac{1}{20}\, dx=\frac1{20}\cdot (20-15)=\frac14=0.25.$$
So, the mass is analogous to the interval ($[a,b]=[15,20]$) and the volume is analogous to the entire range ($[c,d]=[10,30]$). To get a feeling for PDF, consider a continuous random variable $X$ and define the function $f_X(x)$ as follows (wherever the limit exists):
$$f_X(x)=\lim_{\Delta \rightarrow 0^+} \frac{P(x < X \leq x+\Delta)}{\Delta}.$$
The function $f_X(x)$ gives us the probability density at point $x$. It is the limit of the probability of the interval $(x,x+\Delta]$ divided by the length of the interval as the length of the interval goes to $0$. Remember that 
$$P(x<X\leq x+\Delta) = F_X(x+\Delta)-F_X(x).$$
Thus, we get
\begin{align*}
	f_X(x)&=\lim_{\Delta \rightarrow 0} \frac{F_X(x+\Delta)-F_X(x)}{\Delta}\\
		  &= \frac{dF_X(x)}{dx}\\
		  &= F'_X(x), \quad \text{if }F_X(x)\text{ is differentiable at }x.
\end{align*}

Let's find the PDF of the uniform random variable $X\sim Uniform(a,b)$, which can be expressed as follows: 
\begin{equation*}
	f_X(x) = \left\{
	\begin{array}{l l}
	\frac{1}{b-a} & \quad a < x < b\\
	0 & \quad x < a \textrm{ or } x > b
	\end{array} \right.
\end{equation*}
Note that the CDF is not differentiable at points $a$ and $b$. Nevertheless, this is not important at this moment. 

The uniform distribution is the simplest continuous random variable you can imagine. For other types of continuous random variables the PDF is non-uniform. Note that for small values of $\delta$ we can write 
$$P(x < X \leq x+\delta) \approx f_X(x) \delta.$$
Thus, if $f_X(x_1)<f_X(x_2)$, we can say $P(x_1 < X \leq x_1+\delta)<P(x_2 < X \leq x_2+\delta)$ , \ie the value of $X$ is more likely to be around $x_2$ than $x_1$. 

Since the PDF is the derivative of the CDF, the CDF can be obtained from PDF by integrations (by assuming absolute continuity):
$$F_X(x) = \int_{-\infty}^xf_X(u)du.$$
Also, we have
$$P(a < X \leq b) = F_X(b)-F_X(a) = \int_{a}^bf_X(u)du.$$
More generally, for a set $A$, $P(X\in A) = \int_a^b f_X(u)du.$ Note that if we integrate over the entire real line, we must get 1, \ie
$$\int_{-\infty}^{\infty}f_X(u)du = 1.$$

\section{Expected Value and Variance}
As we mentioned earlier, the theory of continuous random variables is very similar to the theory of discrete random variables. In particular, usually summations are replaced by integrals and PMFs are replaced by PDFs. The proofs and ideas are very analogous to the discrete case, so sometimes we state the results without mathematical derivations for the purpose of brevity.

Recall that the expected value of a discrete random variable can be obtained as
$$EX = \sum_{x_k\in R_X}x_k P_X(x_k).$$

The expected value of a continuous RV as
$$EX = \int_{-\infty}^{\infty}x f_X(x)dx.$$

\subsection{Expected Value of a Function of a Continuous Random Variable}
Law of the unconscious statistician (LOTUS) for continuous random variables:
$$\mathbb{E}[g(X)] = \int_{-\infty}^{\infty}g(x) f_X(x)dx$$

\subsection{Variance}
\begin{align*}
	\text{Var}(X) &= \mathbb{E}[(X-\mu_X)^2] \\
				  &= \int_{-\infty}^{\infty}(x-\mu_X)^2 f_X(x)dx\\
				  &= EX^2-(EX)^2\\
				  &= \int_{-\infty}^{\infty}x^2 f_X(x)dx-\mu_X^2
\end{align*}
Note that for $a,b\in \mathbb{R}$, we always have
$$\text{Var}(aX+b) = a^2\text{Var}(X)$$


\section{Functions of Continuous Random Variables}

If $X$ is a continuous random variable and $Y=g(X)$ is a function of $X$, then $Y$ itself is a random variable. Thus, we should be able to find the CDF and PDF of $Y$. It is usually more straightforward to start from the CDF and then to find the PDF by taking the derivative of the CDF. Note that before differentiating the CDF, we should check that the CDF is continuous. As we will see later, the function of a continuous random variable might be a non-continuous random variable. Let's look at an example.

\paragraph{Example:} Let $X$ be a $Uniform(0,1)$ random variable, and let $Y=e^X$.
\begin{itemize}
	\item CDF of $Y$
	\item PDF of $Y$
	\item $EY$
\end{itemize}

The PDF of $X$ is given by

\begin{align*}
	F_X(x) =
	\begin{cases}
		0&\text{for}\ x<0\\
		x&\text{for}\ 0\leq x\leq 1\\
		1&\text{for}\ x>1
	\end{cases}
\end{align*}

The range of $x$, $R_X=[0,1]$, so the range of $Y$, $R_Y=[1,e]$. We can find the CDF of $Y$ as follows:
\begin{align*}
	F_Y(y) &= P(Y\leq y)\\ 
		   &= P(e^X\leq y)\\ 
		   &= P(X\leq \ln y) \quad \text{, since }e^x \text{ is an increasing function.}\\ 
		   &= F_X(\ln y) \quad \text{by definition.}\\
		   &= \ln y \quad \text{, since } F_X(x)=x \text{ for } 0\leq x \leq 1 \text{ and } 0\leq \ln y \leq 1. 
\end{align*}

\section{The Method of Transformations of Random Variables}
So far, we have discussed how we can find the distribution of a function of a continuous random variable starting from finding the CDF. If we are interested in finding the PDF of $Y=g(X)$, where $g(\cdot)$ is some deterministic transformation of $X$, and the function $g$ satisfies following properties, we can utilize a method called the method of transformations.
\begin{itemize}
	\item $g(x)$ is differentiable;
	\item $g(x)$ is a strictly (or monotonically) increasing function, that is, if $x_1<x_2$, then $g(x_1)<g(x_2)$.
\end{itemize}

Now, let $X$ be a continuous random variable and $Y=g(X)$. We will show that you can directly find the PDF of $Y$ using the following formula.
\begin{equation*}
	f_Y(y) = 
	\begin{cases}
	\frac{f_X(x_1)}{g'(x_1)}=f_X(x_1). \frac{dx_1}{dy} & \quad \textrm{where } g(x_1)=y\\
	0 & \quad \textrm{if }g(x)=y \textrm{ does not have a solution}
	\end{cases} 
\end{equation*}

Note that the derivative $\frac{dx}{dy}$ or $\frac{d}{dy}(g^{-1}(y))$ \textbf{measures how $X$ changes with respect to $Y$}.
% Note that start with the function $y=f^{-1}(x)$. Write this as $x=f(y)$ and differentiate both sides implicitly with respect to$x$ using the Chain Rule:
% \begin{align*}
% 	1 &= f'(y)\frac{dy}{dx}\\
% 	\frac{dy}{dx} &= \frac{1}{f'(y)}\\
% 	y &= f^{-1}(x)\\
% 	[f^{-1}]'(x) &= \frac{1}{f'(f^{-1}(x))}
% \end{align*}
Since $g$ is strictly increasing, its inverse function $g^{-1}$ is well defined. You can imagine a simple function like a linear function, \eg $Y=3X+1$. Then, for each $y\in R_Y$, there exists a \textbf{unique} $x_1$ such that $g(x_1)=y$. We can write $x_1=g^{-1}(y)$.
\begin{align*}
	\{Y\leq y\} = \{g(X)\leq y\} = \{X \leq g^{-1}(x)\}.
\end{align*}
Thus, 
\begin{align*}
	F_Y(y) &= P(Y\leq y)\\
		   &= P(g(X)\leq y)\\
		   &= P(X\leq g^{-1}(y))\quad \text{, since }g \text{ is strictly increasing.}\\
		   &= F_X(g^{-1}(y)).
\end{align*}
To find the PDF of $Y$, we differentiate $F_Y(y)$ as follows:
\begin{align*}
	f_Y(y) &= \frac{d}{dy}F_X(x_1)\quad \text{by } g(x_1)=y\\
		   &=\frac{dx_1}{dy}\cdot \underbrace{\frac{d}{dx_1}F_X(x_1)}_{=F'_X(x_1)}\\
		   &=\frac{dx_1}{dy}f_X(x_1)\\
		   &=f_X(g^{-1}(y))\left|\frac{d}{dy}(g^{-1}(y))\right|
		   % &= \frac{f_X(x_1)}{g'(x_1)} \quad \text{, since } \frac{dx}{dy}=\frac{1}{\frac{dy}{dx}}.
\end{align*}
We can repeat the same argument for the case where $g$ is \textbf{strictly decreasing}. In that case, $g'(x_1)$ will be \textbf{negative}, so we need to use $|g'(x_1)|$ . Thus, we can state the following theorem for a \textit{strictly monotonic function}. (A function $g:R\to R$ is called strictly monotonic if it is strictly increasing or strictly decreasing.)

Actually, we assumed that $g$ was one-to-one out of convenience: the condition that $g$ is one-to-one is not necessary for change of variables to work: Consider a continuous random variable $X$ with domain $R_X$, and let $Y=g(X)$. Suppose that we can partition $R_X$ into a finite number of intervals such that $g(x)$ is strictly monotone and differentiable on each partition. Then the PDF of $Y$ is given by 
\begin{align*}
	f_Y(y)= \sum_{i=1}^{n} \frac{f_X(x_i)}{|g'(x_i)|}= \sum_{i=1}^{n} f_X(x_i).
	\left|\frac{dx_i}{dy}\right|,
\end{align*}
where $x_1,\dots,x_n$ are real solutions to $g(x)=y$.

\subsection{Intuitive Explanation}
How to derive the PDF of the random variable $Y=g(X)$ when one knows the PDF of the random variable $X$? If $X$ is discrete, we can derive the pmf for $Y$ by simply summing up the probability mass for all the $x$'s such that $f(x)=y$. For a general function $g$, there is no direct formula to get the PDF of the random variable $Y=g(X)$ knowing $p(X)$. There is a formula in case when $h$ is a differentiable one-to-one mapping from the range (\ie the support) of $X$ to the range of $Y$.

Take for example a random variable $X\sim \mathcal{N}(\mu, \sigma)$ and set $Y=\exp(X)$. The figure below shows some simulations of $X$ and the corresponding values of $Y$. The density of $X$ is shown in blue and the one of $Y$ is shown in orange in the vertical direction.
\begin{figure}[ht]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/sample.png}
        % \caption{Caption for the first figure}
        % \label{fig:figure1}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/sample2.png}
        % \caption{Caption for the second figure}
        % \label{fig:figure2}
    \end{minipage}
    % \caption{Overall caption for the two figures}
    % \label{fig:figures}
\end{figure}
Now the question is: knowing the density of $X$, what is the density of $Y$?
Taking a point $y$ in the range of $Y$, the PDF $f_Y$ provides the probability of $Y$, belong to a small area $dy$ around $y$ by the formula below
$$P(Y\in dy)\approx f_Y(y)|dy|,$$
where $P(Y\in dy)$ is the area below the curve. Similarly, we can define
$$P(X\in dx)\approx f_X(x)|dx|$$
The above two areas are approximately the same in case of very small region. Note that if $dy$ and $dx$ are very small, we can approximate the derivative of $g'(x)=\frac{|dy|}{|dx|}$. Compactly, this can be expressed as follows:
$$P(X\in dx) = f_X(x)\frac{|dy|}{g'(x)}$$
With $y=g(x)$ we can get 
\begin{align*}
	P(X\in dx)\approx P(Y\in dy) &= f_X(x)\frac{|dy|}{g'(x)}\\
	& = f_X(g^{-1}(y))\frac{|dy|}{g'(g^{-1}(y))}\\
	& = f_X(g^{-1}(y))|dy|(g^{-1})'(y)
\end{align*}
The last line is by the derivative of inverse function which is 
$$\frac{d}{dx}f^{-1}(x) = \frac{1}{f'(f^{-1}(x))}$$
Finally, we can get 
$$f_Y(y) = f_X(g^{-1}(y))|(g^{-1})'(y)|$$
Note that the absolute is determined by the function $h$. This is the so-called \textit{change of variables formula}.

\section{Various Distributions}

\subsection{Uniform Distribution}
\begin{equation*}
	f_X(x) = \left\{
	\begin{array}{l l}
	\frac{1}{b-a} & \quad a < x < b\\
	0 & \quad x < a \textrm{ or } x > b
	\end{array} \right.
\end{equation*}

\begin{align*}
	P(c\leq x \leq d) = \int_c^d f(x)dx = \int_c^d \frac{1}{b-a}dx = \frac{d-c}{b-a}
\end{align*}

The expected value of a uniform distribution is
\begin{align*}
	EX = \int_c^d xf(x)dx = \int_c^d \frac{x}{b-a}dx = \frac{b-a}{2}
\end{align*}

The variance of a uniform distribution is given by
\begin{align*}
	\text{Var}(X) &= EX^2-E^2X \\
				  &= \int_c^d \frac{x^2}{b-a} - \left(\frac{b-a}{2}\right)^2dx\\
				  &= \frac{(b-a)^2}{12}
\end{align*}

\subsection{Exponential Distribution}

Check more in \href{https://www.probabilitycourse.com/chapter4/4_2_2_exponential.php}{Exponential Dist.}

A continuous random variable $X$ is said to have an exponential distribution with parameter $\lambda>0$, shown as $X\sim Exponential(\lambda)$, if its PDF is given by 
\begin{align*}
	f_X(x) = \begin{cases}
		\lambda e^{-\lambda x} & \quad x > 0\\
		0 & \quad \textrm{otherwise}
	\end{cases}
\end{align*}


\subsection{Gamma Distribution}
The gamma distribution is another widely used distribution. Its importance is largely due to its relation to exponential and normal distributions. Here, we will provide an introduction to the gamma distribution. Before introducing the gamma random variable, we need to introduce the gamma function. 

\paragraph{Gamma function} $\Gamma(x)$ is an extension of the factorial function to real (and complex) numbers. In specific, if $n\in \{1, 2, 3,\dots\}$, then
$$\Gamma(n) = (n-1)!.$$
More generally, for any positive real number $\alpha$, $\Gamma(\alpha)$ is defined as follows:
$$\Gamma(\alpha) = \int_{0}^{\infty}x^{\alpha-1}e^{-x}dx,\ \text{for }\alpha>0.$$
Note that for $\alpha=1$, 
$$\Gamma(\alpha) = 1.$$

\paragraph{Gamma Distribution} is a distribution with parameters $\alpha>0$ and $\lambda>0$. its PDF is given by
\begin{align*}
	f_X(x) = \begin{cases}
		\frac{\lambda^\alpha x^{\alpha-1} e^{-\lambda x}}{\Gamma(\alpha)}& x>0\\
		0&\text{otherwise}
	\end{cases}
\end{align*}



\section{Mixed Random Variables}
The mixed random variables are random variables that are \textbf{neither discrete nor continuous, but are a mixture of both}.

To find the cumulative distribution function (CDF) of \(Y\), given that \(Y = g(X)\) and the transformation \(g(X)\) is defined as:

\[ 
g(X) = 
\begin{cases} 
X^2 & 0 \leq X \leq \frac{1}{2} \\
2X - 1 & \frac{1}{2} < X \leq 1 
\end{cases}
\]

\begin{enumerate} 
	\item Determine the PDF of \(X\). The given PDF of \(X\) is:
	\[ 
	f_X(x) = 
	\begin{cases} 
	2x & 0 \leq x \leq 1 \\
	0 & \text{otherwise}
	\end{cases}
	\]
\item Determine the ranges of \(Y\). The ranges of \(Y\) are derived from the transformation:
	\begin{itemize}
		\item For \(0 \leq X \leq \frac{1}{2}\):
			\[ Y = X^2 \]
			\[ 0 \leq Y \leq \left(\frac{1}{2}\right)^2 = \frac{1}{4} \]

		\item For \(\frac{1}{2} < X \leq 1\):
			\[ Y = 2X - 1 \]
			\[ 2\left(\frac{1}{2}\right) - 1 < Y \leq 2(1) - 1 \]
			\[ 0 < Y \leq 1 \]
		\end{itemize}
	\item Combining these, we get the range of \(Y\) as \(0 \leq Y \leq 1\). Find the CDF of \(Y\). The CDF of \(Y\), \(F_Y(y)\), is given by \(F_Y(y) = P(Y \leq y)\). We need to consider the two different transformations:
		\begin{enumerate}
			\item For \(0 \leq y \leq \frac{1}{4}\):
				\[ Y = X^2 \]
				\[ P(Y \leq y) = P(X^2 \leq y) = P(X \leq \sqrt{y}) \]
				\[ F_Y(y) = P(X \leq \sqrt{y}) = \int_{0}^{\sqrt{y}} 2x \, dx \]
				\[ F_Y(y) = \left[ x^2 \right]_{0}^{\sqrt{y}} = (\sqrt{y})^2 = y \]
			\item For \(\frac{1}{4} < y \leq 1\):
				\[ Y = 2X - 1 \]
				\[ P(Y \leq y) = P(2X - 1 \leq y) = P(X \leq \frac{y+1}{2}) \]
				\[ F_Y(y) = P(X \leq \frac{y+1}{2}) = \int_{0}^{\frac{y+1}{2}} 2x \, dx \]
				\[ F_Y(y) = \left[ x^2 \right]_{0}^{\frac{y+1}{2}} = \left( \frac{y+1}{2} \right)^2 \]
				\[ F_Y(y) = \frac{(y+1)^2}{4} \]
		\end{enumerate}
	\item Combining these results, the CDF of \(Y\) is:
		\[ 
		F_Y(y) = 
		\begin{cases} 
		y & 0 \leq y \leq \frac{1}{4} \\
		\frac{(y+1)^2}{4} & \frac{1}{4} < y \leq 1
		\end{cases}
		\]
\end{enumerate}

\subsection{Delta Function}
In this section, we will \textbf{use the Dirac delta function to analyze mixed random variables}. Technically speaking, \textit{the Dirac delta function is not actually a function}. It is what we may call a generalized function. Nevertheless, its definition is intuitive and it simplifies dealing with probability distributions.

Remember that any random variable has a CDF. Thus, we can use the CDF to answer questions regarding discrete, continuous, and mixed random variables. On the other hand, the PDF is defined only for continuous random variables, while the PMF is defined only for discrete random variables. Using \textbf{delta functions will allow us to define the PDF for discrete and mixed random variables.} Thus, it allows us to unify the theory of discrete, continuous, and mixed random variables.

\paragraph{Dirac Delta Function}
We cannot define the PDF for a discrete random variable because its CDF has jumps. If we could somehow differentiate the CDF at jump points, we would be able to define the PDF for discrete random variables as well. This is the idea behind our effort in this section. Here, we will introduce the \textit{Dirac delta function} and discuss its application to probability distributions. Let's derive the Dirac delta function. 

First, consider the following unit step function $u(x)$:
\begin{equation*}
	u(x) = \left\{
	\begin{array}{l l}
	1 & \quad x \geq 0 \\
	0 & \quad \text{otherwise}
	\end{array} \right.
\end{equation*}
This function has a discontinuity at $x=0$. Let us remove the jump and define, for any $\alpha>0$, the function $u_\alpha(x)$ as 

\begin{equation*}
	u_{\alpha}(x) = \left\{
	\begin{array}{l l}
	1 & \quad x > \frac{\alpha}{2} \\
	\frac{1}{\alpha} (x+\frac{\alpha}{2}) & \quad -\frac{\alpha}{2} \leq x \leq \frac{\alpha}{2} \\
	0 & \quad x < -\frac{\alpha}{2}
	\end{array} \right.
\end{equation*}

The good thing about $u_\alpha(x)$ is that it is a continuous function. Now let us define the function $\delta_\alpha(x)$ as the derivative of $u_\alpha(x)$ wherever it exists. 

\begin{equation*}
	\delta_{\alpha}(x)=\frac{ d u_{\alpha}(x)}{dx} = \left\{
	\begin{array}{l l}
	\frac{1}{\alpha} & \quad |x| < \frac{\alpha}{2} \\
	0 & \quad |x| > \frac{\alpha}{2}
	\end{array} \right.
\end{equation*}
We can notice that 
\begin{align*}
	\delta_{\alpha}(x)=\frac{d}{dx} u_{\alpha}(x), \quad u(x) \overset{\text{a.e.}}{=} \lim_{\alpha \rightarrow 0} u_{\alpha}(x) \footnotemark
\end{align*}

\footnotetext{The term almost everywhere is abbreviated a.e.; in older literature p.p. is used, to stand for the equivalent French language phrase presque partout.} Now, we would like to define the delta ``function'', $\delta(x)$, as 

$$\delta(x) = \lim_{\alpha \rightarrow 0} \delta_{\alpha}(x).$$
Note that as $\alpha$ becomes smaller and smaller, the height of $\delta_\alpha(x)$ becomes larger and larger and its width becomes smaller and smaller. Taking the limit, we obtain 
\begin{equation*}
	\delta(x) = \left\{
	\begin{array}{l l}
	\infty & \quad x=0 \\
	0 & \quad \text{otherwise}
	\end{array} \right.
\end{equation*}
Equivalently, 
$$\delta(x)=\frac{d}{dx} u(x).$$
Intuitively, with extremely small $\alpha$, we would like to have the following definitions. Let $g: \mathbb{R}\to\mathbb{R}$ be a continuous function. We define
\begin{align*}
	\int_{-\infty}^{\infty} g(x) \delta(x-x_0) dx = \lim_{\alpha \rightarrow 0}
	\bigg[ \int_{-\infty}^{\infty} g(x) \delta_{\alpha} (x-x_0) dx \bigg]
\end{align*}
Then, we have the following lemma, which in fact is the most useful property of the delta function.

Let $g: \mathbb{R}\to\mathbb{R}$ be a continuous function. We have
$$\int_{-\infty}^{\infty} g(x) \delta(x-x_0) dx = g(x_0).$$


\paragraph{Using the Delta Function in PDFs of Discrete and Mixed RV} Consider a discrete random variable $X$ with range $R_X=\{x_1,\dots,x_n\}$ and PMF $P_X(x_k)$. Note that the CDF for X can be written as 
$$F_X(x)=\sum_{x_k \in R_X} P_X(x_k)u(x-x_k).$$
where:
\begin{itemize}
	\item \( u(x - x_k) \) is the Heaviside step function, which is defined as:
	  \[
	  u(x - x_k) = \begin{cases} 
	  0 & \text{if } x < x_k \\
	  1 & \text{if } x \ge x_k 
	  \end{cases}
	  \]
\end{itemize}
The sum \( \sum_{x_k \in \mathbb{R}_X} P_X(x_k) u(x - x_k) \) effectively includes only those \( x_k \) values that are less than or equal to \( x \) due to the step function \( u(x - x_k) \). Therefore, it accumulates the probabilities \( P_X(x_k) \) for all \( x_k \le x \).

Now that we have symbolically defined the derivative of the step function as the delta function, we can write a PDF for $X$ by ``differentiating'' the CDF:

\begin{align*}
	f_X(x) &= \frac{dF_X(x)}{dx}\\ 
		   &= \sum_{x_k\in R_X}P_X(x_k)\frac{d}{dx}u(x-x_k)\\
		   &= \sum_{x_k\in R_X}P_X(x_k)\delta(x-x_k)
\end{align*}
We call this the \textbf{generalized PDF}.
$$EX=\int_{-\infty}^{\infty} xf_X(x)dx.$$
\begin{align*}
	EX &= \int_{-\infty}^{\infty} xf_X(x)dx\\
	   &= \int_{-\infty}^{\infty} x\sum_{x_k\in R_X}P_X(x_k)\delta(x-x_k)dx\\
	   &= \sum_{x_k\in R_X}P_X(x_k)\int_{-\infty}^{\infty}x\delta(x-x_k)dx\\
	   &= \sum_{x_k\in R_X}x_kP_X(x_k)
\end{align*}

